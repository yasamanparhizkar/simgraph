{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1559fcec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasamanparhizkar/Documents/yorku/01_thesis/pyenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import my_simgraph_04 as sg\n",
    "import data_handler_01 as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe49625",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa6039-6b29-497a-a224-14527270328f",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45813d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56ec5b-6c16-444b-b66e-9a229113bd24",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd80e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa076e19-d81d-4bb9-9e5f-72a7c5b4bb1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25ab2-987b-40b8-bee2-a4b14c982c29",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e53a00-7a28-4dbc-96b5-f9590645f836",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6720d07c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_valset(train_data, val_num, ind_min, ind_max, data_params):\n",
    "    minus_set = train_data['smpls']\n",
    "    lbl_func = data_params['lbl_func']\n",
    "    val_num, val_smpls = dh.update_indices_balanced(val_num, ind_min, ind_max, minus_set, lbl_func(data_params), seed=None)\n",
    "    val_dess, val_lbls = dh.update_set(val_smpls, data_params)\n",
    "    val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "    \n",
    "    return val_num, val_data\n",
    "\n",
    "def visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path):\n",
    "    M = B.T @ B\n",
    "    sg.display_matrix(M, None)\n",
    "    # mark prominent elements          \n",
    "    lim = (thresh/100) * np.max(M) # marker threshold                \n",
    "    plt.plot(xloc[M > lim],yloc[M > lim], marker='o', markersize=3, color='r', linestyle='')\n",
    "    plt.title('M - marked above {}%'.format(thresh))\n",
    "    # save figure\n",
    "    plt.savefig(res_path+'finalM_'+str(val_num)+'_'+str(train_num)+'_'+str(train_comb)+'.png')\n",
    "    plt.close()\n",
    "    \n",
    "def assessment_quantities(val_data, val_num, y_est, val_acc):\n",
    "    nospk_per = np.sum(val_data['lbls']==-1)/val_num\n",
    "    min_acc = max(nospk_per, 1-nospk_per)\n",
    "    if sum(val_data['lbls']==1) == 0:\n",
    "        missed = 0\n",
    "    else:\n",
    "        missed = sum(np.logical_and(val_data['lbls']==1, y_est < 0))/sum(val_data['lbls']==1)\n",
    "\n",
    "    if sum(val_data['lbls']==-1) == 0:\n",
    "        false_alarm = 0\n",
    "    else:\n",
    "        false_alarm = sum(np.logical_and(val_data['lbls']==-1, y_est > 0))/sum(val_data['lbls']==-1)\n",
    "        \n",
    "    assess_qs = {'min_acc': min_acc, 'val_acc': val_acc, 'missed': missed, 'false_alarm': false_alarm}\n",
    "        \n",
    "    return assess_qs\n",
    "\n",
    "def make_line(head, train_num, val_num, res_dict, index):\n",
    "    line = '{:^10} | {:^10} | {:^10} | {:^10.2f} | {:^10.2f} | {:^17.2f} | {:^17.2f} \\n'\\\n",
    "           .format(head, train_num, val_num, \\\n",
    "                   res_dict['min_acc'][index]*100, \\\n",
    "                   res_dict['val_acc'][index]*100, \\\n",
    "                   res_dict['missed'][index]*100, \\\n",
    "                   res_dict['false_alarm'][index]*100)\n",
    "    return line\n",
    "\n",
    "def take_train_step(train_num, val_num, ind_min, ind_max, data_params, sg_opt_params, mu, Dt, edges_tt, thresh, xloc, yloc,  train_comb, res_path, seed=None):\n",
    "    # create training set\n",
    "    train_num, _, train_data, _ = dh.random_train_val_balanced(train_num, val_num, ind_min, ind_max, data_params, seed)\n",
    "\n",
    "    # train the model              \n",
    "    B, sg_stats = sg.fit_graph(train_data['des'], train_data['lbls'], sg_opt_params, mu, Dt, seed, edges_tt)\n",
    "\n",
    "    # visualize learned M\n",
    "    visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path)\n",
    "    \n",
    "    return train_num, train_data, B, sg_stats\n",
    "\n",
    "def take_val_step(train_data, val_num, ind_min, ind_max, data_params, B, Dv, Dvt, seed=None):\n",
    "    # create validation set, NO overlap with the training set\n",
    "    val_num, val_data = get_valset(train_data, val_num, ind_min, ind_max, data_params)\n",
    "\n",
    "    # validate the model\n",
    "    val_acc, y_est, t = sg.get_acc(B, train_data['des'], train_data['lbls'], val_data['des'], val_data['lbls'], Dv, Dvt, seed, show_edges=False)\n",
    "\n",
    "    # compute several assessment quantities\n",
    "    assess_qs = assessment_quantities(val_data, val_num, y_est, val_acc)\n",
    "    \n",
    "    return val_num, val_data, assess_qs, y_est, t\n",
    "\n",
    "def avg_and_log(next_dict, prev_dict, index, head, train_num, val_num, func, path):\n",
    "    # compute averages over random combinations of validation sets\n",
    "    for quantity in prev_dict:\n",
    "        if func == 'mean':\n",
    "            next_dict[quantity][index] = np.mean(prev_dict[quantity])\n",
    "        elif func == 'std':\n",
    "            next_dict[quantity][index] = np.std(prev_dict[quantity])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # save on file\n",
    "    with open(path+'log.txt', 'a') as file:\n",
    "        line = make_line(head, train_num, val_num, next_dict, index)\n",
    "        file.write(line)\n",
    "        \n",
    "    return next_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b09316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, mu, Dt, Dv, Dvt, edges_tt, res_path, data_params, sg_opt_params, ind_min, ind_max, thresh, f_sz, xloc, yloc, seed=None):\n",
    "    # prepare results file\n",
    "    if not os.path.exists(res_path):\n",
    "        os.mkdir(res_path)\n",
    "    \n",
    "    with open(res_path+'log.txt', 'w') as file:    \n",
    "        arr = ('{:^10} | {:^10} | {:^10} | {:^10} | {:^10} | {:^17} | {:^17} \\n'\\\n",
    "               .format('i', 'train_num', 'val_num', 'min_acc(%)', 'val_acc(%)',\\\n",
    "                       'missed spks(%)', 'false alarms(%)'),'-'*101+'\\n')\n",
    "        file.writelines(arr)\n",
    "\n",
    "    # create dictionaries to keep interesting variables\n",
    "    assess_qs = {'min_acc': 0, 'val_acc': 0, 'missed': 0, 'false_alarm': 0}\n",
    "    val_comb_res = {}\n",
    "    train_comb_res = {}\n",
    "    train_num_res = {}\n",
    "    train_num_err = {}\n",
    "    val_num_res = {}\n",
    "    val_num_err = {}\n",
    "    for quantity in assess_qs:\n",
    "        val_comb_res[quantity] = np.zeros(val_combs)\n",
    "        train_comb_res[quantity] = np.zeros(train_combs)\n",
    "        train_num_res[quantity] = np.zeros(len(train_sizes))\n",
    "        train_num_err[quantity] = np.zeros(len(train_sizes))\n",
    "        val_num_res[quantity] = np.zeros(len(val_sizes))\n",
    "        val_num_err[quantity] = np.zeros(len(val_sizes))\n",
    "\n",
    "    i = 0\n",
    "    for val_num in val_sizes:\n",
    "        j = 0\n",
    "        for train_num in train_sizes:\n",
    "            for train_comb in range(train_combs):\n",
    "                # train\n",
    "                train_num, train_data, B, sg_stats = \\\n",
    "                take_train_step(train_num, val_num, ind_min, ind_max, data_params, sg_opt_params, \n",
    "                                mu, Dt, edges_tt, thresh, xloc, yloc, train_comb, res_path, seed)\n",
    "\n",
    "                for val_comb in range(val_combs):\n",
    "                    # validate\n",
    "                    val_num, val_data, assess_qs, y_est, t = \\\n",
    "                    take_val_step(train_data, val_num, ind_min, ind_max, data_params, B, Dv, Dvt, seed)\n",
    "                    # log resutls\n",
    "                    val_comb_res = avg_and_log(val_comb_res, assess_qs, val_comb, str(val_comb), train_num, val_num, 'mean', res_path)\n",
    "\n",
    "                # average over various validation set combinations and log\n",
    "                train_comb_res = avg_and_log(train_comb_res, val_comb_res, train_comb, '>'+str(train_comb), train_num, val_num, 'mean', res_path)\n",
    "            # average over various training and validation set combinations and log\n",
    "            train_num_res = avg_and_log(train_num_res, train_comb_res, j, '*t*', train_num, val_num, 'mean', res_path)\n",
    "            train_num_err = avg_and_log(train_num_err, train_comb_res, j, '*te*', train_num, val_num, 'std', res_path)\n",
    "            j += 1\n",
    "        # average over various training set sizes and training and validation set combinations, and log\n",
    "        val_num_res = avg_and_log(val_num_res, train_num_res, i, '**v**', train_num, val_num, 'mean', res_path)\n",
    "        val_num_err = avg_and_log(val_num_err, train_num_res, i, '**ve**', train_num, val_num, 'std', res_path)\n",
    "        i += 1   \n",
    "        # save train_num_res curves for this specific val_num\n",
    "        with open(res_path+'curves_'+str(i-1)+'.txt', 'w') as file:\n",
    "            for quantity in assess_qs:\n",
    "                np.savetxt(file, train_num_res[quantity])\n",
    "                np.savetxt(file, train_num_err[quantity])\n",
    "                file.write('\\n')\n",
    "        \n",
    "    return val_num_res, val_num_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396aeda9-57a2-4187-944b-25c1b90006d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path):\n",
    "    curves = {}\n",
    "    errors = {}\n",
    "    for i in range(len(val_sizes)):\n",
    "        curves_i = np.loadtxt(res_path+'curves_'+str(i)+'.txt')\n",
    "        curves_i = curves_i.reshape(8, -1)\n",
    "        j = 0\n",
    "        for quantity in val_num_res:\n",
    "            if i == 0:\n",
    "                curves[quantity] = curves_i[2*j].reshape(1, -1)\n",
    "                errors[quantity] = curves_i[2*j+1].reshape(1, -1)\n",
    "            else:\n",
    "                curves[quantity] = np.concatenate((curves[quantity], [curves_i[2*j]]), axis=0)\n",
    "                errors[quantity] = np.concatenate((errors[quantity], [curves_i[2*j+1]]), axis=0)\n",
    "            j += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.8,hspace=0.8)\n",
    "    for i in range(len(val_sizes)):\n",
    "        plt.subplot(len(val_sizes), 1, i+1)\n",
    "        for quantity in curves:\n",
    "            plt.errorbar(train_sizes, curves[quantity][i], errors[quantity][i])\n",
    "        plt.legend(curves.keys())\n",
    "        plt.xlabel('training set size')\n",
    "        plt.ylabel('{} val repeats x {} train repeats'.format(val_combs, train_combs))\n",
    "        _ = plt.title('val. set size = {}, Dt = {}, Dvt = {}, Dv = {}, $\\mu$ = {}'.format(val_sizes[i], 'None', 2000, 0, 30))\n",
    "    plt.savefig(res_path+'train_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    for quantity in val_num_res:\n",
    "        plt.errorbar(val_sizes, val_num_res[quantity], val_num_err[quantity])\n",
    "    plt.legend(val_num_res.keys())\n",
    "    plt.xlabel('validation set size')\n",
    "    plt.ylabel('{} val repeats x {} train repeats x {} train set sizes'.format(val_combs, train_combs, len(train_sizes)))\n",
    "    _ = plt.title('Dt = {}, Dvt = {}, Dv = {}, $\\mu$ = {}'.format('None', 2000, 0, 30))\n",
    "    plt.savefig(res_path+'val_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afde5ec-587e-45d1-b3db-3d3ad78748a7",
   "metadata": {},
   "source": [
    "## Assess with various $\\mu$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03442d6e-dfae-4b33-bee6-a4b5bc828a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only consider the second trial\n",
    "ind_min = 1*1141+0\n",
    "ind_max = 2*1141-1\n",
    "\n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': dh.datapoint_torch, 'lbl_func': dh.get_labels, 'features_dp': '../../../data/features/slowfast/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform}\n",
    "\n",
    "sg_opt_params = { 'epsilon0':1, 'epsilon_decay':0.5, 'epsilon_jump': 2, \\\n",
    "                'num_its':16, 'check_freq':1, 'print_checks':False, 'Theta0':None, \\\n",
    "                'force_all_its': True, 'threshold': 0.01}\n",
    "\n",
    "# try various training and validation set sizes\n",
    "# train_sizes = [50, 100, 150, 200, 250, 300]\n",
    "train_sizes = [6, 7]\n",
    "# val_sizes = [10]\n",
    "val_sizes = [6]\n",
    "\n",
    "# for each set size, try a number of random combinations of datapoints\n",
    "train_combs = 5\n",
    "val_combs = 10\n",
    "\n",
    "thresh = 30\n",
    "f_sz = 474 # must match data_params\n",
    "xloc = np.broadcast_to(np.arange(f_sz), (f_sz, f_sz))\n",
    "yloc = xloc.T\n",
    "\n",
    "# mu = 30\n",
    "Dt = 50\n",
    "Dv = 0\n",
    "Dvt = 26\n",
    "edges_tt = None\n",
    "\n",
    "org_path = '../../../data/experiments/slowfast/slowfast_sg/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb32f59-072f-4078-924d-8523416c1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu_list = [30]\n",
    "for mu in mu_list:\n",
    "    print('mu = ', mu)\n",
    "    res_path = org_path+'mu_'+str(mu)+'/'\n",
    "    val_num_res, val_num_err = assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, mu, Dt, Dv, Dvt, edges_tt, res_path, data_params, sg_opt_params, ind_min, ind_max, thresh, f_sz, xloc, yloc)\n",
    "    plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6b54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac8c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
