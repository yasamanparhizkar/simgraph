{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a47073-247c-4b6a-bc43-0f2b6a20a0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasamanparhizkar/Documents/yorku/01_thesis/pyenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import my_simgraph_04 as sg\n",
    "import compare_with_benchmark as bn\n",
    "import data_handler_01 as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d48a33-5409-4787-9821-ee00a302cc24",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6e5d2-64f3-4c20-9727-a1e6acd02a61",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4861da2b-23f6-41fe-8447-46cc60ff2cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd59ee0-8f39-4ebf-8cd0-fdfbeebef0da",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee2c183-5785-4667-a2c5-ac97ccd58ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a63567-9529-433a-a6c5-92fae2521920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a56d8-cd57-42cd-92c4-8657b8d9e926",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378a3bc-f950-4d94-abf5-36e1099b61c1",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21194d4-fdbd-4aac-af83-2b41a41b8fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_valset(train_data, val_num, ind_min, ind_max, data_params):\n",
    "    # prevent overlap with training set\n",
    "    minus_set = train_data['smpls']\n",
    "    \n",
    "    # create datapoints' label vector for a balanced set\n",
    "    spike_data = data_params['spike_data']\n",
    "    group_id = data_params['group_id']\n",
    "    lbls = spike_data[:,:,group_id].reshape(-1)\n",
    "    \n",
    "    val_num, val_smpls = dh.update_indices_balanced(val_num, ind_min, ind_max, minus_set, lbls, seed=None)\n",
    "    val_dess, val_lbls = dh.update_set(val_smpls, data_params)\n",
    "    val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "    \n",
    "    return val_num, val_data\n",
    "\n",
    "def visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path):\n",
    "    M = B.T @ B\n",
    "    sg.display_matrix(M, None)\n",
    "    # mark prominent elements          \n",
    "    lim = (thresh/100) * np.max(M) # marker threshold                \n",
    "    plt.plot(xloc[M > lim],yloc[M > lim], marker='o', markersize=3, color='r', linestyle='')\n",
    "    plt.title('M - marked above {}%'.format(thresh))\n",
    "    # save figure\n",
    "    plt.savefig(res_path+'finalM_'+str(val_num)+'_'+str(train_num)+'_'+str(train_comb)+'.png')\n",
    "    plt.close()\n",
    "    \n",
    "def assessment_quantities(val_data, val_num, y_est, val_acc):\n",
    "    nospk_per = np.sum(val_data['lbls']==-1)/val_num\n",
    "    min_acc = max(nospk_per, 1-nospk_per)\n",
    "    if sum(val_data['lbls']==1) == 0:\n",
    "        missed = 0\n",
    "    else:\n",
    "        missed = sum(np.logical_and(val_data['lbls']==1, y_est < 0))/sum(val_data['lbls']==1)\n",
    "\n",
    "    if sum(val_data['lbls']==-1) == 0:\n",
    "        false_alarm = 0\n",
    "    else:\n",
    "        false_alarm = sum(np.logical_and(val_data['lbls']==-1, y_est > 0))/sum(val_data['lbls']==-1)\n",
    "        \n",
    "    assess_qs = {'min_acc': min_acc, 'val_acc': val_acc, 'missed': missed, 'false_alarm': false_alarm}\n",
    "        \n",
    "    return assess_qs\n",
    "\n",
    "def make_line(head, train_num, val_num, res_dict, index):\n",
    "    line = '{:^10} | {:^10} | {:^10} | {:^10.2f} | {:^10.2f} | {:^17.2f} | {:^17.2f} \\n'\\\n",
    "           .format(head, train_num, val_num, \\\n",
    "                   res_dict['min_acc'][index]*100, \\\n",
    "                   res_dict['val_acc'][index]*100, \\\n",
    "                   res_dict['missed'][index]*100, \\\n",
    "                   res_dict['false_alarm'][index]*100)\n",
    "    return line\n",
    "\n",
    "def take_train_step(train_num, val_num, ind_min, ind_max, data_params, nn_opt_params, nn_arch_params, seed=None):\n",
    "    # create training set\n",
    "    train_num, _, train_data, _ = dh.random_train_val_balanced(train_num, val_num, ind_min, ind_max, data_params, seed)\n",
    "\n",
    "    # train the model              \n",
    "    Theta, nn_stats = bn.fit(train_data['des'], (train_data['lbls'] == 1).astype(int), nn_opt_params, nn_arch_params, show_nrmdE=False)\n",
    "    \n",
    "    return train_num, train_data, Theta, nn_stats\n",
    "\n",
    "def take_val_step(train_data, val_num, ind_min, ind_max, data_params, nn_arch_params, Theta, seed=None):\n",
    "    # create validation set, NO overlap with the training set\n",
    "    val_num, val_data = get_valset(train_data, val_num, ind_min, ind_max, data_params)\n",
    "\n",
    "    # validate the model\n",
    "    val_acc, y_est = bn.get_acc(val_data['des'], (val_data['lbls'] == 1).astype(int), nn_arch_params, Theta)\n",
    "    y_est = y_est * 2 - 1\n",
    "    # compute several assessment quantities\n",
    "    assess_qs = assessment_quantities(val_data, val_num, y_est, val_acc)\n",
    "    \n",
    "    return val_num, val_data, assess_qs, y_est\n",
    "\n",
    "def avg_and_log(next_dict, prev_dict, index, head, train_num, val_num, func, path):\n",
    "    # compute averages over random combinations of validation sets\n",
    "    for quantity in prev_dict:\n",
    "        if func == 'mean':\n",
    "            next_dict[quantity][index] = np.mean(prev_dict[quantity])\n",
    "        elif func == 'std':\n",
    "            next_dict[quantity][index] = np.std(prev_dict[quantity])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # save on file\n",
    "    with open(path+'log.txt', 'a') as file:\n",
    "        line = make_line(head, train_num, val_num, next_dict, index)\n",
    "        file.write(line)\n",
    "        \n",
    "    return next_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5aece18-3012-4ac0-8ea1-f5b584a1662a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_nn_model(train_sizes, val_sizes, train_combs, val_combs, res_path, data_params, nn_opt_params, nn_arch_params, ind_min, ind_max, seed=None):\n",
    "    # prepare results file\n",
    "    with open(res_path+'log.txt', 'w') as file:    \n",
    "        arr = ('{:^10} | {:^10} | {:^10} | {:^10} | {:^10} | {:^17} | {:^17} \\n'\\\n",
    "               .format('i', 'train_num', 'val_num', 'min_acc(%)', 'val_acc(%)',\\\n",
    "                       'missed spks(%)', 'false alarms(%)'),'-'*101+'\\n')\n",
    "        file.writelines(arr)\n",
    "\n",
    "    # create dictionaries to keep interesting variables\n",
    "    assess_qs = {'min_acc': 0, 'val_acc': 0, 'missed': 0, 'false_alarm': 0}\n",
    "    val_comb_res = {}\n",
    "    train_comb_res = {}\n",
    "    train_num_res = {}\n",
    "    train_num_err = {}\n",
    "    val_num_res = {}\n",
    "    val_num_err = {}\n",
    "    for quantity in assess_qs:\n",
    "        val_comb_res[quantity] = np.zeros(val_combs)\n",
    "        train_comb_res[quantity] = np.zeros(train_combs)\n",
    "        train_num_res[quantity] = np.zeros(len(train_sizes))\n",
    "        train_num_err[quantity] = np.zeros(len(train_sizes))\n",
    "        val_num_res[quantity] = np.zeros(len(val_sizes))\n",
    "        val_num_err[quantity] = np.zeros(len(val_sizes))\n",
    "\n",
    "    i = 0\n",
    "    for val_num in val_sizes:\n",
    "        j = 0\n",
    "        for train_num in train_sizes:\n",
    "            for train_comb in range(train_combs):\n",
    "                # train\n",
    "                train_num, train_data, Theta, nn_stats = \\\n",
    "                take_train_step(train_num, val_num, ind_min, ind_max, data_params, nn_opt_params, nn_arch_params, seed)\n",
    "                \n",
    "\n",
    "                for val_comb in range(val_combs):\n",
    "                    # validate\n",
    "                    val_num, val_data, assess_qs, y_est= \\\n",
    "                    take_val_step(train_data, val_num, ind_min, ind_max, data_params, nn_arch_params, Theta, seed)\n",
    "                    # log resutls\n",
    "                    val_comb_res = avg_and_log(val_comb_res, assess_qs, val_comb, str(val_comb), train_num, val_num, 'mean', res_path)\n",
    "\n",
    "                # average over various validation set combinations and log\n",
    "                train_comb_res = avg_and_log(train_comb_res, val_comb_res, train_comb, '>'+str(train_comb), train_num, val_num, 'mean', res_path)\n",
    "            # average over various training and validation set combinations and log\n",
    "            train_num_res = avg_and_log(train_num_res, train_comb_res, j, '*t*', train_num, val_num, 'mean', res_path)\n",
    "            train_num_err = avg_and_log(train_num_err, train_comb_res, j, '*te*', train_num, val_num, 'std', res_path)\n",
    "            j += 1\n",
    "        # average over various training set sizes and training and validation set combinations, and log\n",
    "        val_num_res = avg_and_log(val_num_res, train_num_res, i, '**v**', train_num, val_num, 'mean', res_path)\n",
    "        val_num_err = avg_and_log(val_num_err, train_num_res, i, '**ve**', train_num, val_num, 'std', res_path)\n",
    "        i += 1   \n",
    "        # save train_num_res curves for this specific val_num\n",
    "        with open(res_path+'curves_'+str(i-1)+'.txt', 'w') as file:\n",
    "            for quantity in assess_qs:\n",
    "                np.savetxt(file, train_num_res[quantity])\n",
    "                np.savetxt(file, train_num_err[quantity])\n",
    "                file.write('\\n')\n",
    "        \n",
    "    return val_num_res, val_num_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76f2799-6d0d-4853-9ca4-88b92e0f1075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path, nn_arch_params):\n",
    "    curves = {}\n",
    "    errors = {}\n",
    "    for i in range(len(val_sizes)):\n",
    "        curves_i = np.loadtxt(res_path+'curves_'+str(i)+'.txt')\n",
    "        curves_i = curves_i.reshape(8, -1)\n",
    "        j = 0\n",
    "        for quantity in val_num_res:\n",
    "            if i == 0:\n",
    "                curves[quantity] = curves_i[2*j].reshape(1, -1)\n",
    "                errors[quantity] = curves_i[2*j+1].reshape(1, -1)\n",
    "            else:\n",
    "                curves[quantity] = np.concatenate((curves[quantity], [curves_i[2*j]]), axis=0)\n",
    "                errors[quantity] = np.concatenate((errors[quantity], [curves_i[2*j+1]]), axis=0)\n",
    "            j += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.8,hspace=0.8)\n",
    "    for i in range(len(val_sizes)):\n",
    "        plt.subplot(len(val_sizes), 1, i+1)\n",
    "        for quantity in curves:\n",
    "            plt.errorbar(train_sizes, curves[quantity][i], errors[quantity][i])\n",
    "        plt.legend(curves.keys())\n",
    "        plt.xlabel('training set size')\n",
    "        plt.ylabel('{} val repeats x {} train repeats'.format(val_combs, train_combs))\n",
    "        _ = plt.title('val. set size = {}, #layers = {}, #units = {}'.format(val_sizes[i], nn_arch_params['num_hidden_layers'], nn_arch_params['num_hidden_units']))\n",
    "    plt.savefig(res_path+'train_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    for quantity in val_num_res:\n",
    "        plt.errorbar(val_sizes, val_num_res[quantity], val_num_err[quantity])\n",
    "    plt.legend(val_num_res.keys())\n",
    "    plt.xlabel('validation set size')\n",
    "    plt.ylabel('{} val repeats x {} train repeats x {} train set sizes'.format(val_combs, train_combs, len(train_sizes)))\n",
    "    _ = plt.title('#layers = {}, #units = {}'.format(nn_arch_params['num_hidden_layers'], nn_arch_params['num_hidden_units']))\n",
    "    plt.savefig(res_path+'val_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75620faf-ee52-400f-8246-a61b67f90c4d",
   "metadata": {},
   "source": [
    "## Assess with various $\\mu$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2dd25ad-9971-48c5-996f-7e45598c8ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only consider the second trial\n",
    "ind_min = 1*1141+0\n",
    "ind_max = 2*1141-1\n",
    "\n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': dh.datapoint_torch, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/slowfast/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform}\n",
    "\n",
    "nn_opt_params = { 'epsilon0':1, 'epsilon_decay':0.5, 'epsilon_jump': 2, \\\n",
    "                  'num_its':16, 'check_freq':1, 'print_checks':False, 'Theta0':None, \\\n",
    "                  'force_all_its': True, 'threshold': 0.01}\n",
    "\n",
    "num_classes = 2\n",
    "nn_arch_params = { 'num_hidden_layers':5, 'num_hidden_units':20, 'num_outs':num_classes, \\\n",
    "               'act_func':bn.relu, 'out_func':bn.softmax, 'loss_func':bn.crossent }\n",
    "\n",
    "# try various training and validation set sizes\n",
    "train_sizes = [10, 20, 30, 40, 50, 100, 150, 200, 250, 300]\n",
    "val_sizes = [10, 20]\n",
    "\n",
    "# for each set size, try a number of random combinations of datapoints\n",
    "train_combs = 10\n",
    "val_combs = 10\n",
    "\n",
    "res_path = '../../data/experiments/slowfast/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a1e1cb0-a056-4605-a514-633d74a1f1ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_num_res, val_num_err = assess_nn_model(train_sizes, val_sizes, train_combs, val_combs, res_path, data_params, nn_opt_params, nn_arch_params, ind_min, ind_max, seed=None)\n",
    "plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path, nn_arch_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d663e-416e-411c-87db-11f697edcd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1ffd1-68af-46e8-91e1-1a718dbb9b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb283d-e521-424f-aa59-e218c1e73c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376650d-f557-4859-931f-7c7620cdc5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed68968-cef6-4876-b171-88df0b099050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdf6c3-40ec-44d7-8a09-be8a2d02ec52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da85be8-9c59-4d98-91f5-8c08435dbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "658c54ef-82c7-4cc5-89d9-354f51434a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_handler_01' from '/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages/data_handler_01.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload a package\n",
    "import importlib\n",
    "importlib.reload(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53de0ef-c69b-4b1a-84e1-51dbf1b369c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
