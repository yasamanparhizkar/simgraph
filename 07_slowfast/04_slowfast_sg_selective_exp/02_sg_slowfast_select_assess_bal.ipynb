{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a47073-247c-4b6a-bc43-0f2b6a20a0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasamanparhizkar/Documents/yorku/01_thesis/pyenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import my_simgraph_04 as sg\n",
    "import compare_with_benchmark as bn\n",
    "import data_handler_01 as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d48a33-5409-4787-9821-ee00a302cc24",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6e5d2-64f3-4c20-9727-a1e6acd02a61",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4861da2b-23f6-41fe-8447-46cc60ff2cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd59ee0-8f39-4ebf-8cd0-fdfbeebef0da",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee2c183-5785-4667-a2c5-ac97ccd58ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a63567-9529-433a-a6c5-92fae2521920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a56d8-cd57-42cd-92c4-8657b8d9e926",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378a3bc-f950-4d94-abf5-36e1099b61c1",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21194d4-fdbd-4aac-af83-2b41a41b8fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_valset(train_data, val_num, ind_min, ind_max, data_params):\n",
    "    # prevent overlap with training set\n",
    "    minus_set = train_data['smpls']\n",
    "    \n",
    "    # create datapoints' label vector for a balanced set\n",
    "    lbl_func = data_params['lbl_func']\n",
    "    lbls = lbl_func(data_params)\n",
    "    \n",
    "    val_num, val_smpls = dh.update_indices_balanced(val_num, ind_min, ind_max, minus_set, lbls, seed=None)\n",
    "    val_dess, val_lbls = dh.update_set(val_smpls, data_params)\n",
    "    val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "    \n",
    "    return val_num, val_data\n",
    "\n",
    "def visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path):\n",
    "    M = B.T @ B\n",
    "    sg.display_matrix(M, None)\n",
    "    # mark prominent elements          \n",
    "    # lim = (thresh/100) * np.max(M) # marker threshold          \n",
    "    lim = np.percentile(M, thresh)\n",
    "    plt.plot(xloc[M > lim],yloc[M > lim], marker='o', markersize=3, color='r', linestyle='')\n",
    "    plt.title('M - marked above {}%'.format(thresh))\n",
    "    # save figure\n",
    "    if not os.path.exists(res_path+'pic/'):\n",
    "        os.mkdir(res_path+'pic/')\n",
    "    plt.savefig(res_path+'pic/finalM_'+str(val_num)+'_'+str(train_num)+'_'+str(train_comb)+'.png')    \n",
    "    # save matrix\n",
    "    if not os.path.exists(res_path+'mat/'):\n",
    "        os.mkdir(res_path+'mat/')\n",
    "    np.savetxt(res_path+'mat/finalM_'+str(val_num)+'_'+str(train_num)+'_'+str(train_comb)+'.txt', M)\n",
    "    plt.close()\n",
    "    \n",
    "def assessment_quantities(val_data, val_num, y_est, val_acc):\n",
    "    nospk_per = np.sum(val_data['lbls']!=1)/val_num\n",
    "    min_acc = max(nospk_per, 1-nospk_per)\n",
    "    if sum(val_data['lbls']==1) == 0:\n",
    "        missed = 0\n",
    "    else:\n",
    "        missed = sum(np.logical_and(val_data['lbls']==1, y_est < 0))/sum(val_data['lbls']==1)\n",
    "\n",
    "    if sum(val_data['lbls']!=1) == 0:\n",
    "        false_alarm = 0\n",
    "    else:\n",
    "        false_alarm = sum(np.logical_and(val_data['lbls']!=1, y_est > 0))/sum(val_data['lbls']!=1)\n",
    "        \n",
    "    assess_qs = {'min_acc': min_acc, 'val_acc': val_acc, 'missed': missed, 'false_alarm': false_alarm}\n",
    "        \n",
    "    return assess_qs\n",
    "\n",
    "def make_line(head, train_num, val_num, res_dict, index):\n",
    "    line = '{:^10} | {:^10} | {:^10} | {:^10.2f} | {:^10.2f} | {:^17.2f} | {:^17.2f} \\n'\\\n",
    "           .format(head, train_num, val_num, \\\n",
    "                   res_dict['min_acc'][index]*100, \\\n",
    "                   res_dict['val_acc'][index]*100, \\\n",
    "                   res_dict['missed'][index]*100, \\\n",
    "                   res_dict['false_alarm'][index]*100)\n",
    "    return line\n",
    "\n",
    "def take_train_step(train_num, val_num, ind_min, ind_max, data_params, sg_opt_params,\n",
    "                    mu, Dt, edges_tt, thresh, xloc, yloc,  train_comb, res_path, seed=None):\n",
    "    # create training set\n",
    "    train_num, _, train_data, _ = dh.random_train_val_balanced(train_num, val_num, ind_min, ind_max, data_params, seed)\n",
    "    \n",
    "    # train the model              \n",
    "    B, sg_stats = sg.fit_graph(train_data['des'], train_data['lbls'], sg_opt_params, mu, Dt, seed, edges_tt)\n",
    "\n",
    "    # visualize learned M\n",
    "    visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path)\n",
    "    \n",
    "    return train_num, train_data, B, sg_stats\n",
    "\n",
    "def take_val_step(train_data, val_num, ind_min, ind_max, data_params, B, Dv, Dvt, thresh, seed=None):\n",
    "    # create validation set, NO overlap with the training set\n",
    "    val_num, val_data = get_valset(train_data, val_num, ind_min, ind_max, data_params)\n",
    "\n",
    "    # validate the model with full M\n",
    "    val_acc, y_est, t = sg.get_acc(B, train_data['des'], train_data['lbls'], val_data['des'], val_data['lbls'], \n",
    "                                   Dv, Dvt, seed=seed, show_edges=False)\n",
    "    \n",
    "    # validate the model with only high valued entries of M\n",
    "    val_acc_s, y_est_s, t_s = sg.get_acc(B, train_data['des'], train_data['lbls'], val_data['des'], val_data['lbls'], \n",
    "                                   Dv, Dvt, selective=True, thresh=thresh, seed=seed, show_edges=False)\n",
    "\n",
    "    # compute several assessment quantities\n",
    "    assess_qs = assessment_quantities(val_data, val_num, y_est, val_acc)\n",
    "    assess_qs_s = assessment_quantities(val_data, val_num, y_est_s, val_acc_s)\n",
    "    \n",
    "    return val_num, val_data, assess_qs, assess_qs_s, y_est\n",
    "\n",
    "def avg_and_log(next_dict, prev_dict, index, head, train_num, val_num, func, path):\n",
    "    # compute averages over random combinations of validation sets\n",
    "    for quantity in prev_dict:\n",
    "        if func == 'mean':\n",
    "            next_dict[quantity][index] = np.mean(prev_dict[quantity])\n",
    "        elif func == 'std':\n",
    "            next_dict[quantity][index] = np.std(prev_dict[quantity])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # save on file\n",
    "    with open(path+'log.txt', 'a') as file:\n",
    "        line = make_line(head, train_num, val_num, next_dict, index)\n",
    "        file.write(line)\n",
    "        \n",
    "    return next_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5aece18-3012-4ac0-8ea1-f5b584a1662a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, mu, Dt, Dv, Dvt, edges_tt, res_path, data_params, sg_opt_params, ind_min, ind_max, thresh, f_sz, xloc, yloc, seed=None):\n",
    "    # prepare results file\n",
    "    if not os.path.exists(res_path):\n",
    "        os.mkdir(res_path)\n",
    "    with open(res_path+'log.txt', 'w') as file:    \n",
    "        arr = ('{:^10} | {:^10} | {:^10} | {:^10} | {:^10} | {:^17} | {:^17} \\n'\\\n",
    "               .format('i', 'train_num', 'val_num', 'min_acc(%)', 'val_acc(%)',\\\n",
    "                       'missed spks(%)', 'false alarms(%)'),'-'*101+'\\n')\n",
    "        file.writelines(arr)\n",
    "\n",
    "    # create dictionaries to keep interesting variables\n",
    "    assess_qs = {'min_acc': 0, 'val_acc': 0, 'missed': 0, 'false_alarm': 0}\n",
    "    val_comb_res = {}\n",
    "    train_comb_res = {}\n",
    "    train_num_res = {}\n",
    "    train_num_err = {}\n",
    "    val_num_res = {}\n",
    "    val_num_err = {}\n",
    "    # selectivity related dicts\n",
    "    val_comb_res_s = {}\n",
    "    train_comb_res_s = {}\n",
    "    train_num_res_s = {}\n",
    "    train_num_err_s = {}\n",
    "    val_num_res_s = {}\n",
    "    val_num_err_s = {}\n",
    "    for quantity in assess_qs:\n",
    "        val_comb_res[quantity] = np.zeros(val_combs)\n",
    "        train_comb_res[quantity] = np.zeros(train_combs)\n",
    "        train_num_res[quantity] = np.zeros(len(train_sizes))\n",
    "        train_num_err[quantity] = np.zeros(len(train_sizes))\n",
    "        val_num_res[quantity] = np.zeros(len(val_sizes))\n",
    "        val_num_err[quantity] = np.zeros(len(val_sizes))\n",
    "        # selectivity related\n",
    "        val_comb_res_s[quantity] = np.zeros(val_combs)\n",
    "        train_comb_res_s[quantity] = np.zeros(train_combs)\n",
    "        train_num_res_s[quantity] = np.zeros(len(train_sizes))\n",
    "        train_num_err_s[quantity] = np.zeros(len(train_sizes))\n",
    "        val_num_res_s[quantity] = np.zeros(len(val_sizes))\n",
    "        val_num_err_s[quantity] = np.zeros(len(val_sizes))\n",
    "\n",
    "    i = 0\n",
    "    for val_num in val_sizes:\n",
    "        j = 0\n",
    "        for train_num in train_sizes:\n",
    "            for train_comb in range(train_combs):\n",
    "                # train\n",
    "                train_num, train_data, B, sg_stats = \\\n",
    "                take_train_step(train_num, val_num, ind_min, ind_max, data_params, sg_opt_params, \n",
    "                                mu, Dt, edges_tt, thresh, xloc, yloc, train_comb, res_path, seed)\n",
    "                \n",
    "                for val_comb in range(val_combs):\n",
    "                    # validate\n",
    "                    val_num, val_data, assess_qs, assess_qs_s, y_est = \\\n",
    "                    take_val_step(train_data, val_num, ind_min, ind_max, data_params, B, Dv, Dvt, thresh, seed)\n",
    "                    # log resutls\n",
    "                    val_comb_res = avg_and_log(val_comb_res, assess_qs, val_comb, str(val_comb), train_num, val_num, 'mean', res_path)\n",
    "                    val_comb_res_s = avg_and_log(val_comb_res_s, assess_qs_s, val_comb, '~'+str(val_comb), train_num, val_num, 'mean', res_path)\n",
    "\n",
    "                # average over various validation set combinations and log\n",
    "                train_comb_res = avg_and_log(train_comb_res, val_comb_res, train_comb, '>'+str(train_comb), train_num, val_num, 'mean', res_path)\n",
    "                train_comb_res_s = avg_and_log(train_comb_res_s, val_comb_res_s, train_comb, '>~'+str(train_comb), train_num, val_num, 'mean', res_path)\n",
    "\n",
    "            # average over various training and validation set combinations and log\n",
    "            train_num_res = avg_and_log(train_num_res, train_comb_res, j, '*t*0', train_num, val_num, 'mean', res_path)\n",
    "            train_num_err = avg_and_log(train_num_err, train_comb_res, j, '*te*0', train_num, val_num, 'std', res_path)\n",
    "            train_num_res_s = avg_and_log(train_num_res_s, train_comb_res_s, j, '*t*'+'~0', train_num, val_num, 'mean', res_path)\n",
    "            train_num_err_s = avg_and_log(train_num_err_s, train_comb_res_s, j, '*te*'+'~0', train_num, val_num, 'std', res_path)\n",
    "            j += 1\n",
    "        # average over various training set sizes and training and validation set combinations, and log\n",
    "        val_num_res = avg_and_log(val_num_res, train_num_res, i, '**v**0', train_num, val_num, 'mean', res_path)\n",
    "        val_num_err = avg_and_log(val_num_err, train_num_res, i, '**ve**0', train_num, val_num, 'std', res_path)\n",
    "        val_num_res_s = avg_and_log(val_num_res_s, train_num_res_s, i, '**v**'+'~0', train_num, val_num, 'mean', res_path)\n",
    "        val_num_err_s = avg_and_log(val_num_err_s, train_num_res_s, i, '**ve**'+'~0', train_num, val_num, 'std', res_path)\n",
    "        i += 1   \n",
    "        # save train_num_res curves for this specific val_num\n",
    "        with open(res_path+'curves_'+str(i-1)+'.txt', 'w') as file:\n",
    "            for quantity in assess_qs:\n",
    "                np.savetxt(file, train_num_res[quantity])\n",
    "                np.savetxt(file, train_num_err[quantity])\n",
    "                file.write('\\n')\n",
    "        \n",
    "        if not os.path.exists(res_path+'selective/'):\n",
    "            os.mkdir(res_path+'selective/')\n",
    "        with open(res_path+'selective/curves_'+str(i-1)+'.txt', 'w') as file:\n",
    "            for quantity in assess_qs:\n",
    "                np.savetxt(file, train_num_res_s[quantity])\n",
    "                np.savetxt(file, train_num_err_s[quantity])\n",
    "                file.write('\\n')\n",
    "        \n",
    "    return val_num_res, val_num_err, val_num_res_s, val_num_err_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c76f2799-6d0d-4853-9ca4-88b92e0f1075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path):\n",
    "    curves = {}\n",
    "    errors = {}\n",
    "    for i in range(len(val_sizes)):\n",
    "        curves_i = np.loadtxt(res_path+'curves_'+str(i)+'.txt')\n",
    "        curves_i = curves_i.reshape(8, -1)\n",
    "        j = 0\n",
    "        for quantity in val_num_res:\n",
    "            if i == 0:\n",
    "                curves[quantity] = curves_i[2*j].reshape(1, -1)\n",
    "                errors[quantity] = curves_i[2*j+1].reshape(1, -1)\n",
    "            else:\n",
    "                curves[quantity] = np.concatenate((curves[quantity], [curves_i[2*j]]), axis=0)\n",
    "                errors[quantity] = np.concatenate((errors[quantity], [curves_i[2*j+1]]), axis=0)\n",
    "            j += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.8,hspace=0.8)\n",
    "    for i in range(len(val_sizes)):\n",
    "        plt.subplot(len(val_sizes), 1, i+1)\n",
    "        for quantity in curves:\n",
    "            plt.errorbar(train_sizes, curves[quantity][i], errors[quantity][i])\n",
    "        plt.legend(curves.keys())\n",
    "        plt.xlabel('training set size')\n",
    "        plt.ylabel('{} val repeats x {} train repeats'.format(val_combs, train_combs))\n",
    "        _ = plt.title('val. set size = {}, Dt = {}, Dvt = {}, Dv = {}, $\\mu$ = {}'.format(val_sizes[i], 'None', 2000, 0, 30))\n",
    "    plt.savefig(res_path+'train_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    for quantity in val_num_res:\n",
    "        plt.errorbar(val_sizes, val_num_res[quantity], val_num_err[quantity])\n",
    "    plt.legend(val_num_res.keys())\n",
    "    plt.xlabel('validation set size')\n",
    "    plt.ylabel('{} val repeats x {} train repeats x {} train set sizes'.format(val_combs, train_combs, len(train_sizes)))\n",
    "    _ = plt.title('Dt = {}, Dvt = {}, Dv = {}, $\\mu$ = {}'.format('None', 2000, 0, 30))\n",
    "    plt.savefig(res_path+'val_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75620faf-ee52-400f-8246-a61b67f90c4d",
   "metadata": {},
   "source": [
    "## Assess with various $\\mu$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2387ba2-6944-4576-8935-215ba3e45614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only consider the second trial\n",
    "ind_min = 1*1141+0\n",
    "ind_max = 2*1141-1\n",
    "    \n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': dh.datapoint_torch, 'lbl_func': dh.get_labels, \n",
    "               'features_dp': '../../data/features/slowfast/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform}\n",
    "\n",
    "sg_opt_params = { 'epsilon0':1, 'epsilon_decay':0.5, 'epsilon_jump': 2, \\\n",
    "              'num_its':10, 'check_freq':17, 'print_checks':False, 'Theta0':None, \\\n",
    "              'force_all_its': True, 'threshold': 0.01}\n",
    "\n",
    "# try various training and validation set sizes\n",
    "train_sizes = [10, 20, 30, 40, 50, 100, 150]\n",
    "val_sizes = [10]\n",
    "\n",
    "# for each set size, try a number of random combinations of datapoints\n",
    "train_combs = 3\n",
    "val_combs = 33\n",
    "\n",
    "thresh = 50\n",
    "f_sz = 474 # must match data_params\n",
    "xloc = np.broadcast_to(np.arange(f_sz), (f_sz, f_sz))\n",
    "yloc = xloc.T\n",
    "\n",
    "mu = 30\n",
    "Dt = None\n",
    "Dv = 0\n",
    "Dvt = 2000\n",
    "edges_tt = None\n",
    "\n",
    "res_path = '../../data/experiments/slowfast/slowfast_sg/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1e1cb0-a056-4605-a514-633d74a1f1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m val_num_res, val_num_err, val_num_res_s, val_num_err_s \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 2\u001b[0m \u001b[43massess_sg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_combs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_combs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDvt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_tt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43msg_opt_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path)\n\u001b[1;32m      6\u001b[0m plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselective/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [6], line 48\u001b[0m, in \u001b[0;36massess_sg_model\u001b[0;34m(train_sizes, val_sizes, train_combs, val_combs, mu, Dt, Dv, Dvt, edges_tt, res_path, data_params, sg_opt_params, ind_min, ind_max, thresh, f_sz, xloc, yloc, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_num \u001b[38;5;129;01min\u001b[39;00m train_sizes:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_comb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_combs):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         train_num, train_data, B, sg_stats \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mtake_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg_opt_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_tt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_comb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val_comb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(val_combs):\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[1;32m     53\u001b[0m             val_num, val_data, assess_qs, assess_qs_s, y_est \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     54\u001b[0m             take_val_step(train_data, val_num, ind_min, ind_max, data_params, B, Dv, Dvt, thresh, seed)\n",
      "Cell \u001b[0;32mIn [14], line 65\u001b[0m, in \u001b[0;36mtake_train_step\u001b[0;34m(train_num, val_num, ind_min, ind_max, data_params, sg_opt_params, mu, Dt, edges_tt, thresh, xloc, yloc, train_comb, res_path, seed)\u001b[0m\n\u001b[1;32m     62\u001b[0m train_num, _, train_data, _ \u001b[38;5;241m=\u001b[39m dh\u001b[38;5;241m.\u001b[39mrandom_train_val_balanced(train_num, val_num, ind_min, ind_max, data_params, seed)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# train the model              \u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m B, sg_stats \u001b[38;5;241m=\u001b[39m \u001b[43msg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlbls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msg_opt_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_tt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# visualize learned M\u001b[39;00m\n\u001b[1;32m     68\u001b[0m visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path)\n",
      "File \u001b[0;32m~/Documents/yorku/01_thesis/code/my_packages/my_simgraph_04.py:438\u001b[0m, in \u001b[0;36mfit_graph\u001b[0;34m(dess, lbls, opt_params, mu, Dt, seed, edges_tt)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcnstr_glr_wrap\u001b[39m(B, deriv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnstr_glr(B, deriv, mu\u001b[38;5;241m=\u001b[39mmu, x\u001b[38;5;241m=\u001b[39mlbls, F\u001b[38;5;241m=\u001b[39mdess\u001b[38;5;241m.\u001b[39mT, edges_tt\u001b[38;5;241m=\u001b[39medges_tt)\n\u001b[0;32m--> 438\u001b[0m B, stats \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnstr_glr_wrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m B, stats\n",
      "File \u001b[0;32m~/Documents/yorku/01_thesis/code/my_packages/my_simgraph_04.py:331\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(loss_func, opt_params, show_nrmdE)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Compute loss and its derivative with current parameter values\u001b[39;00m\n\u001b[1;32m    330\u001b[0m s_start_t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 331\u001b[0m E, dEdTheta \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderiv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m eval_times\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m s_start_t)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Find epsilon which decreases train loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/yorku/01_thesis/code/my_packages/my_simgraph_04.py:436\u001b[0m, in \u001b[0;36mfit_graph.<locals>.cnstr_glr_wrap\u001b[0;34m(B, deriv)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcnstr_glr_wrap\u001b[39m(B, deriv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcnstr_glr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlbls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges_tt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges_tt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/yorku/01_thesis/code/my_packages/my_simgraph_04.py:256\u001b[0m, in \u001b[0;36mcnstr_glr\u001b[0;34m(B, deriv, mu, x, F, edges_tt)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(s, C):\n\u001b[0;32m--> 256\u001b[0m         der \u001b[38;5;241m=\u001b[39m wij \u001b[38;5;241m*\u001b[39m temp[s] \u001b[38;5;241m*\u001b[39m \u001b[43mtemp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    257\u001b[0m         drdM[s,t] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  der\n\u001b[1;32m    258\u001b[0m         drdM[t,s] \u001b[38;5;241m=\u001b[39m drdM[s,t]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_num_res, val_num_err, val_num_res_s, val_num_err_s = \\\n",
    "assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, \n",
    "                mu, Dt, Dv, Dvt, edges_tt, res_path, data_params, \n",
    "                sg_opt_params, ind_min, ind_max, thresh, f_sz, xloc, yloc)\n",
    "plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path)\n",
    "plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path+'selective/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da85be8-9c59-4d98-91f5-8c08435dbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658c54ef-82c7-4cc5-89d9-354f51434a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reload a package\n",
    "# import importlib\n",
    "# importlib.reload(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53de0ef-c69b-4b1a-84e1-51dbf1b369c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
