{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4698f821-b6f5-45da-9bcf-ddd3da6c1135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasamanparhizkar/Documents/yorku/01_thesis/pyenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import dataprocess.data_handler_01 as dh\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d02662-8dc0-49e8-8ea4-31f7f996ee92",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2e0620-97f1-49c0-8f93-c6a566c254c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n"
     ]
    }
   ],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]\n",
    "\n",
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1\n",
    "        \n",
    "print('grouped_data.shape = ', grouped_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b42ccf-2902-4771-8912-0a88ba0f11e7",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec57c3-b86f-47cc-bc4d-3532d4830ab9",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5177941-7bad-4c70-a3bf-6c3eca6a4771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_valset(train_data, val_num, ind_min, ind_max, data_params):\n",
    "    # prevent overlap with training set\n",
    "    minus_set = train_data['smpls']\n",
    "    \n",
    "    # create datapoints' label vector for a balanced set\n",
    "    lbl_func = data_params['lbl_func']\n",
    "    lbls = lbl_func(data_params)\n",
    "    \n",
    "    val_num, val_smpls = dh.update_indices_balanced(val_num, ind_min, ind_max, minus_set, lbls, seed=None)\n",
    "    val_dess, val_lbls = dh.update_set(val_smpls, data_params)\n",
    "    val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "    \n",
    "    return val_num, val_data\n",
    "\n",
    "def visualize_M(B, thresh, xloc, yloc, train_comb, train_num, val_num, res_path):\n",
    "    M = B.T @ B\n",
    "    sg.display_matrix(M, None)\n",
    "    # mark prominent elements          \n",
    "    lim = (thresh/100) * np.max(M) # marker threshold                \n",
    "    plt.plot(xloc[M > lim],yloc[M > lim], marker='o', markersize=3, color='r', linestyle='')\n",
    "    plt.title('M - marked above {}%'.format(thresh))\n",
    "    # save figure\n",
    "    plt.savefig(res_path+'finalM_'+str(val_num)+'_'+str(train_num)+'_'+str(train_comb)+'.png')\n",
    "    plt.close()\n",
    "    \n",
    "def assessment_quantities(val_data, val_num, preds, val_acc):\n",
    "    nospk_per = np.sum(val_data['lbls']!=1)/val_num\n",
    "    min_acc = max(nospk_per, 1-nospk_per)\n",
    "    if sum(val_data['lbls']==1) == 0:\n",
    "        missed = 0\n",
    "    else:\n",
    "        missed = sum(np.logical_and(val_data['lbls']==1, preds < 0))/sum(val_data['lbls']==1)\n",
    "\n",
    "    if sum(val_data['lbls']!=1) == 0:\n",
    "        false_alarm = 0\n",
    "    else:\n",
    "        false_alarm = sum(np.logical_and(val_data['lbls']!=1, preds > 0))/sum(val_data['lbls']!=1)\n",
    "        \n",
    "    assess_qs = {'min_acc': min_acc, 'val_acc': val_acc, 'missed': missed, 'false_alarm': false_alarm}\n",
    "        \n",
    "    return assess_qs\n",
    "\n",
    "def make_line(head, train_num, val_num, res_dict, index):\n",
    "    line = '{:^10} | {:^10} | {:^10} | {:^10.2f} | {:^10.2f} | {:^17.2f} | {:^17.2f} \\n'\\\n",
    "           .format(head, train_num, val_num, \\\n",
    "                   res_dict['min_acc'][index]*100, \\\n",
    "                   res_dict['val_acc'][index]*100, \\\n",
    "                   res_dict['missed'][index]*100, \\\n",
    "                   res_dict['false_alarm'][index]*100)\n",
    "    return line\n",
    "\n",
    "def take_train_step(train_num, val_num, ind_min, ind_max, data_params, knn, res_path, seed=None):\n",
    "    # create training set\n",
    "    train_num, val_num, train_data, val_data = dh.random_train_val_balanced(train_num, val_num, ind_min, ind_max, data_params, seed)\n",
    "\n",
    "    # train the model  \n",
    "    knn.fit(train_data['des'], train_data['lbls'])\n",
    " \n",
    "    return train_num, train_data, knn\n",
    "\n",
    "def take_val_step(train_data, val_num, ind_min, ind_max, data_params, knn, seed=None):\n",
    "    # create validation set, NO overlap with the training set\n",
    "    val_num, val_data = get_valset(train_data, val_num, ind_min, ind_max, data_params)\n",
    "\n",
    "    # validate the model\n",
    "    preds = knn.predict(val_data['des'])\n",
    "    val_acc = knn.score(val_data['des'], val_data['lbls'])\n",
    "\n",
    "    # compute several assessment quantities\n",
    "    assess_qs = assessment_quantities(val_data, val_num, preds, val_acc)\n",
    "    \n",
    "    return val_num, val_data, assess_qs, preds\n",
    "\n",
    "def avg_and_log(next_dict, prev_dict, index, head, train_num, val_num, func, path):\n",
    "    # compute averages over random combinations of validation sets\n",
    "    for quantity in prev_dict:\n",
    "        if func == 'mean':\n",
    "            next_dict[quantity][index] = np.mean(prev_dict[quantity])\n",
    "        elif func == 'std':\n",
    "            next_dict[quantity][index] = np.std(prev_dict[quantity])\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    # save on file\n",
    "    with open(path+'log.txt', 'a') as file:\n",
    "        line = make_line(head, train_num, val_num, next_dict, index)\n",
    "        file.write(line)\n",
    "        \n",
    "    return next_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c5dc8a-7cb4-4cb8-850d-4e1effd5d071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, res_path, data_params, knn_params, ind_min, ind_max, seed=None):\n",
    "    # prepare results file\n",
    "    with open(res_path+'log.txt', 'w') as file:    \n",
    "        arr = ('{:^10} | {:^10} | {:^10} | {:^10} | {:^10} | {:^17} | {:^17} \\n'\\\n",
    "               .format('i', 'train_num', 'val_num', 'min_acc(%)', 'val_acc(%)',\\\n",
    "                       'missed spks(%)', 'false alarms(%)'),'-'*101+'\\n')\n",
    "        file.writelines(arr)\n",
    "\n",
    "    # create dictionaries to keep interesting variables\n",
    "    assess_qs = {'min_acc': 0, 'val_acc': 0, 'missed': 0, 'false_alarm': 0}\n",
    "    val_comb_res = {}\n",
    "    train_comb_res = {}\n",
    "    train_num_res = {}\n",
    "    train_num_err = {}\n",
    "    val_num_res = {}\n",
    "    val_num_err = {}\n",
    "    for quantity in assess_qs:\n",
    "        val_comb_res[quantity] = np.zeros(val_combs)\n",
    "        train_comb_res[quantity] = np.zeros(train_combs)\n",
    "        train_num_res[quantity] = np.zeros(len(train_sizes))\n",
    "        train_num_err[quantity] = np.zeros(len(train_sizes))\n",
    "        val_num_res[quantity] = np.zeros(len(val_sizes))\n",
    "        val_num_err[quantity] = np.zeros(len(val_sizes))\n",
    "    \n",
    "    # create the knn object\n",
    "    n_neighbors = knn_params['n_neighbors']\n",
    "    weights = knn_params['weights']\n",
    "    algorithm = knn_params['algorithm']\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "    \n",
    "    i = 0\n",
    "    for val_num in val_sizes:\n",
    "        j = 0\n",
    "        for train_num in train_sizes:\n",
    "            for train_comb in range(train_combs):\n",
    "                # train               \n",
    "                train_num, train_data, knn = \\\n",
    "                take_train_step(train_num, val_num, ind_min, ind_max, data_params, knn, res_path, seed=None)\n",
    "\n",
    "                for val_comb in range(val_combs):\n",
    "                    # validate\n",
    "                    val_num, val_data, assess_qs, preds = \\\n",
    "                    take_val_step(train_data, val_num, ind_min, ind_max, data_params, knn, seed=None)\n",
    "                    # log resutls\n",
    "                    val_comb_res = avg_and_log(val_comb_res, assess_qs, val_comb, str(val_comb), train_num, val_num, 'mean', res_path)\n",
    "\n",
    "                # average over various validation set combinations and log\n",
    "                train_comb_res = avg_and_log(train_comb_res, val_comb_res, train_comb, '>'+str(train_comb), train_num, val_num, 'mean', res_path)\n",
    "            # average over various training and validation set combinations and log\n",
    "            train_num_res = avg_and_log(train_num_res, train_comb_res, j, '*t*', train_num, val_num, 'mean', res_path)\n",
    "            train_num_err = avg_and_log(train_num_err, train_comb_res, j, '*te*', train_num, val_num, 'std', res_path)\n",
    "            j += 1\n",
    "        # average over various training set sizes and training and validation set combinations, and log\n",
    "        val_num_res = avg_and_log(val_num_res, train_num_res, i, '**v**', train_num, val_num, 'mean', res_path)\n",
    "        val_num_err = avg_and_log(val_num_err, train_num_res, i, '**ve**', train_num, val_num, 'std', res_path)\n",
    "        i += 1   \n",
    "        # save train_num_res curves for this specific val_num\n",
    "        with open(res_path+'curves_'+str(i-1)+'.txt', 'w') as file:\n",
    "            for quantity in assess_qs:\n",
    "                np.savetxt(file, train_num_res[quantity])\n",
    "                np.savetxt(file, train_num_err[quantity])\n",
    "                file.write('\\n')\n",
    "        \n",
    "    return val_num_res, val_num_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9914f7-5725-4df4-89f5-efdf6739e5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path):\n",
    "    curves = {}\n",
    "    errors = {}\n",
    "    for i in range(len(val_sizes)):\n",
    "        curves_i = np.loadtxt(res_path+'curves_'+str(i)+'.txt')\n",
    "        curves_i = curves_i.reshape(8, -1)\n",
    "        j = 0\n",
    "        for quantity in val_num_res:\n",
    "            if i == 0:\n",
    "                curves[quantity] = curves_i[2*j].reshape(1, -1)\n",
    "                errors[quantity] = curves_i[2*j+1].reshape(1, -1)\n",
    "            else:\n",
    "                curves[quantity] = np.concatenate((curves[quantity], [curves_i[2*j]]), axis=0)\n",
    "                errors[quantity] = np.concatenate((errors[quantity], [curves_i[2*j+1]]), axis=0)\n",
    "            j += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.8,hspace=0.8)\n",
    "    for i in range(len(val_sizes)):\n",
    "        plt.subplot(len(val_sizes), 1, i+1)\n",
    "        for quantity in curves:\n",
    "            plt.errorbar(train_sizes, curves[quantity][i], errors[quantity][i])\n",
    "        plt.legend(curves.keys())\n",
    "        plt.xlabel('training set size')\n",
    "        plt.ylabel('{} val repeats x {} train repeats'.format(val_combs, train_combs))\n",
    "        _ = plt.title('val. set size = {}'.format(val_sizes[i]))\n",
    "    plt.savefig(res_path+'train_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    for quantity in val_num_res:\n",
    "        plt.errorbar(val_sizes, val_num_res[quantity], val_num_err[quantity])\n",
    "    plt.legend(val_num_res.keys())\n",
    "    plt.xlabel('validation set size')\n",
    "    plt.ylabel('{} val repeats x {} train repeats x {} train set sizes'.format(val_combs, train_combs, len(train_sizes)))\n",
    "    _ = plt.title('{}'.format('Default hyperparameters'))\n",
    "    plt.savefig(res_path+'val_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d9b91fa-283c-43c7-802a-9de7d2ffe70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only consider the second trial\n",
    "ind_min = 1*1141+0\n",
    "ind_max = 2*1141-1\n",
    "\n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': dh.datapoint_torch, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/slowfast/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform}\n",
    "\n",
    "# Define hyperparameters\n",
    "knn_params = {'n_neighbors': 1, 'weights': 'uniform', 'algorithm': 'auto'}\n",
    "\n",
    "# try various training and validation set sizes\n",
    "train_sizes = [50, 100, 150, 200, 250, 300]\n",
    "val_sizes = [10, 20]\n",
    "\n",
    "# for each set size, try a number of random combinations of datapoints\n",
    "train_combs = 10\n",
    "val_combs = 10\n",
    "\n",
    "res_path = '../../data/experiments/slowfast/slowfast_knn/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "136ea9e7-e4ea-4771-9677-5a9ff5f77887",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num_res, val_num_err = assess_sg_model(train_sizes, val_sizes, train_combs, val_combs, res_path, data_params, knn_params, ind_min, ind_max, seed=None)\n",
    "plot_curves(train_sizes, val_sizes, val_num_res, val_num_err, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a89496-44d3-4a88-a786-dab78783fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078aa99-fcb0-4ea1-bbdd-522542223a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1a76e-72bc-497d-a6f6-95d03bc9ed22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286fdb8-4748-4a1e-889d-60c4cde88951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beafd7-e45b-45bf-b09e-69382d72ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45602d22-eea3-4c31-8f9b-c3c6fd4f5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10d275-58f4-4835-8588-700a32dd6fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd1923-1d94-4b31-b853-39d550169f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dd835-26a5-436c-be2e-bf910bd929a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539f0b2-22c7-4fd7-be5d-cc23a4a8559c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6e511-bc14-4e52-865d-8703950b22ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5ec22-079d-4ed1-92c9-98cbd0a9bba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5b1e1-0617-482e-b907-477bac9a873f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf02f9-fda6-49b6-a735-d183744491cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6088a5-cc02-4517-b998-f30b53153a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc34609-ea6a-42bc-b051-4afe8300a7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd3131-3f4e-4fd9-b2ed-1010a94d81b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17d69d-3604-49f9-b760-a463c9416fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec635f6-7480-4262-9cef-7638f241fa93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121fc67-9073-46ad-ae04-f922c828ad1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca95d7f-95d6-4f81-b770-2e08f22781cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43732d42-e77d-411b-a432-3f8dc2b5184a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e1a4e-7bf5-411a-a396-e1f286d9d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c00a7-f369-4000-8fe6-286ff86d5792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20e798-099b-4798-a26a-3d508bfabc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f4bd6-3b7f-43e6-9242-8a51196bce88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e4b6b-dace-4d8a-9384-a86c86f0499e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26cab8-af7e-475d-b315-c2a5ce457251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa43718-18a1-4df2-ac91-ff592f70e355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29400c5b-42db-4cb7-99ea-7c517a345db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba5ac0-8a4b-4e21-9070-f0df6fcd94c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b686d-797c-454d-9a25-4ef0888ba7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85da828-069f-4580-891b-4861c574af06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
