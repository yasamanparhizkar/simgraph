{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1559fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import data_handler_01 as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe49625",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa6039-6b29-497a-a224-14527270328f",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45813d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56ec5b-6c16-444b-b66e-9a229113bd24",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa076e19-d81d-4bb9-9e5f-72a7c5b4bb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb4900",
   "metadata": {},
   "source": [
    "# Load single datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac8dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to use when creating train and val datasets\n",
    "def datapoint(index, features_dp, spike_data, group_id, transform=None):\n",
    "    \"\"\"\n",
    "    Return a single datapoint consisting of (feature vector, label) \n",
    "    based on the extended index system of the whole dataset (297 repeats of a 1141-frame movie); \n",
    "    for example, the 6th frame of the 7th repeat is indexed 7*1141+5. \n",
    "    In this system, indices only move forward after repeats, so they represent time in a sense.\n",
    "    Acceptable index range is batch_sz-1 to 1141*297-1.\n",
    "      \n",
    "    Inputs: index, features_dp, spike_data, group_id\n",
    "    index - chosen datapoint's index\n",
    "    features_dp - path to where feature vectors are stored.\n",
    "    spike_data  - ndarray containing spike data.\n",
    "                  the array's shape is (297 x 1141 x m) where m is the number of subgroups of neurons.\n",
    "                  subgroups can be a single neuron or as large as all 113 neurons.\n",
    "    group_id    - index of the chosen subgroup of neurons which is being considered\n",
    "    transform   - func. applied to the original feature vector (defult: None, no transform is applied)\n",
    "            \n",
    "    \n",
    "    Output: fv, lbl\n",
    "    fv  - torch tensor representing the selected time bin's feature vector\n",
    "    lbl - the selected time bin's label\n",
    "    \"\"\"\n",
    "    \n",
    "    trial = index//1141\n",
    "    frame = index%1141\n",
    "    fv = torch.load(features_dp+'fv_'+str(frame)+'.pt')\n",
    "    if transform is not None:\n",
    "        fv = transform(fv)\n",
    "    lbl = spike_data[trial, frame, group_id]\n",
    "    \n",
    "    return fv, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacc913",
   "metadata": {},
   "source": [
    "## last layer features & single-neuron spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbee9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint #1146:\n",
      "label =  -1.0\n",
      "feature vector shape =  torch.Size([1, 461])\n"
     ]
    }
   ],
   "source": [
    "# chosen neuron is conveyed via 'group_id' in data_params\n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': datapoint, \\\n",
    "               'features_dp': '../../data/slowfast_4608/', \\\n",
    "               'spike_data': binned_data, \\\n",
    "               'group_id': I_order_10[0], \\\n",
    "               'transform': transform}\n",
    "\n",
    "datapoint_func = data_params['func']\n",
    "features_dp = data_params['features_dp']\n",
    "spike_data = data_params['spike_data']\n",
    "group_id = data_params['group_id']\n",
    "transform = data_params['transform']\n",
    "index = 1*1141+5\n",
    "\n",
    "fv, lbl = datapoint_func(index, features_dp, spike_data, group_id, transform)\n",
    "print('datapoint #{}:'.format(index))\n",
    "print('label = ', lbl)\n",
    "print('feature vector shape = ', fv.shape)\n",
    "# print('feature vector = ', fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9c638",
   "metadata": {},
   "source": [
    "## first layer features & grouped-neurons spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46bce970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint #1146:\n",
      "label =  1.0\n",
      "feature vector shape =  torch.Size([1, 474])\n"
     ]
    }
   ],
   "source": [
    "data_params = {'func': datapoint, \\\n",
    "               'features_dp': '../../data/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, \\\n",
    "               'group_id': 0, \\\n",
    "               'transform': transform}\n",
    "\n",
    "datapoint_func = data_params['func']\n",
    "features_dp = data_params['features_dp']\n",
    "spike_data = data_params['spike_data']\n",
    "group_id = data_params['group_id']\n",
    "transform = data_params['transform']\n",
    "index = 1*1141+5\n",
    "\n",
    "fv, lbl = datapoint_func(index, features_dp, spike_data, group_id, transform)\n",
    "print('datapoint #{}:'.format(index))\n",
    "print('label = ', lbl)\n",
    "print('feature vector shape = ', fv.shape)\n",
    "# print('feature vector = ', fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea96b1",
   "metadata": {},
   "source": [
    "# Create training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae6d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_set(samples, data_params):\n",
    "    \"\"\"\n",
    "    Update a set (either training or validation) based on a new list of datapoints' indices.\n",
    "    \n",
    "    Input: samples, data_params\n",
    "    samples - list of datapoints' indices (original indices representing time)\n",
    "    data_params   -\n",
    "        func - funtion which returns a datapoint (fv, lbl) based on its index\n",
    "        features_dp - path to where feature vectors are stored\n",
    "        spike_data - (297 x 1141 x m)-shaped array where m is the number of subgroups of neurons.\n",
    "        group_id - index of the chosen subgroup of neurons which is being considered\n",
    "        transform - func. applied to the original feature vector (defult: None, no transform is applied)\n",
    "        \n",
    "    Output: dess, lbls\n",
    "    dess - NxD matrix of feature vectors of N datapoints\n",
    "    lbls - Nx1 vector of corresponding labels of said datapoints\n",
    "    \"\"\"\n",
    "    \n",
    "    datapoint = data_params['func']\n",
    "    features_dp = data_params['features_dp']\n",
    "    spike_data = data_params['spike_data']\n",
    "    group_id = data_params['group_id']\n",
    "    transform = data_params['transform'] if 'transform' in data_params else None\n",
    "    \n",
    "    dess = []\n",
    "    lbls = []\n",
    "    for index in samples:\n",
    "        fv, lbl = datapoint(index, features_dp, spike_data, group_id, transform)\n",
    "        dess.append(fv)\n",
    "        lbls.append(lbl)\n",
    "    dess = torch.cat(dess)\n",
    "    dess = dess.detach().numpy()\n",
    "    lbls = np.array(lbls)\n",
    "    \n",
    "    return dess, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2053f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_train_val(train_num, val_num, ind_min, ind_max, data_params, seed=None):\n",
    "    \"\"\"\n",
    "    Choose random datapoints to form training and validation datasets. The two sets do not overlap.\n",
    "    Note: since datapoints are selected randomly, their new indices do NOT represent time anymore.\n",
    "    \n",
    "    Input: train_num, val_num, ind_min, ind_max, data_params, seed\n",
    "    train_num     - size of the training datase\n",
    "    val_num       - size of the validation dataset\n",
    "    ind_min       - minimum possible datapoint index (acceptable >= batch_sz-1)\n",
    "    ind_max       - maximum possible datapoint index (acceptable < 297*1141)\n",
    "    data_params   -\n",
    "        func - funtion which returns a datapoint (fv, lbl) based on its index\n",
    "        features_dp - path to where feature vectors are stored\n",
    "        spike_data - (297 x 1141 x m)-shaped array where m is the number of subgroups of neurons.\n",
    "        group_id - index of the chosen subgroup of neurons which is being considered\n",
    "        transform - func. applied to the original feature vector (defult: None, no transform is applied)\n",
    "    seed - for random selection of datapoints (default: None, machine chosen seed is used)\n",
    "    \n",
    "    Output: train_num, val_num, train_data, val_data\n",
    "    train_num  - number of training datapoints\n",
    "    val_num    - number of validation datapoints\n",
    "    train_data - \n",
    "        des   - NxD numpy array of feature vectors\n",
    "        lbls  - Nx1 numpy array of corresponding labels\n",
    "        smpls - list of indices of chosen datapoints, original indices which represent time\n",
    "    val_data  - \n",
    "        des   - NxD numpy array of feature vectors\n",
    "        lbls  - Nx1 numpy array of corresponding labels\n",
    "        smpls - list of indices of chosen datapoints, original indices which represent time\n",
    "    \"\"\"\n",
    "    \n",
    "    data_num = ind_max - ind_min + 1\n",
    "    train_num = min(train_num, data_num)\n",
    "    val_num   = min(val_num, data_num-train_num)\n",
    "    \n",
    "    # select indices of datapoints randomly\n",
    "    rng = np.random.default_rng(seed)\n",
    "    samples = rng.choice(np.arange(ind_min, ind_max+1), size=(train_num+val_num), replace=False)\n",
    "    train_smpls = samples[:train_num]\n",
    "    val_smpls   = samples[train_num:]\n",
    "    \n",
    "    # get feature vectors and labels corresponding to chosen indices\n",
    "    train_dess, train_lbls = update_set(train_smpls, data_params)\n",
    "    val_dess, val_lbls = update_set(val_smpls, data_params)\n",
    "    \n",
    "    train_data = {'des': train_dess, 'lbls': train_lbls, 'smpls': train_smpls}\n",
    "    val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "    \n",
    "    return train_num, val_num, train_data, val_data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8bc47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num =  10 , val_num =  10\n",
      "training data contains 7 points (70.00%) of label 1.\n",
      "validation data contains 6 points (60.00%) of label 1.\n"
     ]
    }
   ],
   "source": [
    "# only consider the second trial\n",
    "ind_min = 1*1141+0\n",
    "ind_max = 2*1141-1\n",
    "# train_num = int(data_num*0.8)\n",
    "# val_num = data_num - train_num\n",
    "train_num = 10\n",
    "val_num = 10\n",
    "\n",
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[:, ::10]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "data_params = {'func': datapoint, 'features_dp': '../../data/slowfast_4732/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform}\n",
    "\n",
    "train_num, val_num, train_data, val_data = \\\n",
    "random_train_val(train_num, val_num, ind_min, ind_max, data_params, seed=1342)\n",
    "\n",
    "# show statistics\n",
    "print('train_num = ', train_num, ', val_num = ', val_num)\n",
    "print('training data contains {} points ({:.2f}%) of label 1.'\n",
    "      .format(np.sum(train_data['lbls'] == 1), np.sum(train_data['lbls'] == 1)*100/train_num))\n",
    "print('validation data contains {} points ({:.2f}%) of label 1.'\n",
    "      .format(np.sum(val_data['lbls'] == 1), np.sum(val_data['lbls'] == 1)*100/val_num))\n",
    "\n",
    "# print('train_smpls = ', train_data['smpls'], '\\nval_smpls = ', val_data['smpls'])\n",
    "# print('train_lbls = ', train_data['lbls'], '\\nval_lbls = ', val_data['lbls'])\n",
    "# print('train_des = ', train_data['des'], '\\nval_des = ', val_data['des'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42726c3",
   "metadata": {},
   "source": [
    "## snippet to update a set (training or validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32324eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indices(num, ind_min, ind_max, minus_set, seed=None):\n",
    "    \"\"\"\n",
    "    Update the choice of datapoints for a specific set (either training or validation).\n",
    "    Do not consider indices in the minus_set as options; \n",
    "    this prevents overlap between the previous and the new sets, or between the training and validation sets.\n",
    "    \n",
    "    Input: num, ind_min, ind_max, minus_set, seed=None\n",
    "    num - number of datapoints in the final set\n",
    "    ind_min - minimum possible datapoint index (acceptable >= batch_sz-1)\n",
    "    ind_max - maximum possible datapoint index (acceptable < 297*1141)\n",
    "    minus_set - set of discarded indices (AKA overlapping indices)\n",
    "    seed - for random selection of datapoints (default: None, machine chosen seed is used)\n",
    "    \n",
    "    Output: num, samples\n",
    "    num - number of chosen datapoints\n",
    "    samples - list of chosen datapoints' indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove overlapping indices from options\n",
    "    options = np.arange(ind_min, ind_max+1)\n",
    "    keeplist = [True] * len(options)\n",
    "    for index in minus_set:\n",
    "        keeplist = np.logical_and(keeplist, (options != index))\n",
    "    options = options[keeplist]\n",
    "    \n",
    "    # user error: more datapoints are requested that available options\n",
    "    num = min(num, len(options))\n",
    "    \n",
    "    # select randomly\n",
    "    rng = np.random.default_rng(seed)\n",
    "    samples = rng.choice(options, size=num, replace=False)\n",
    "    \n",
    "    return num, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a685952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new val_num =  15\n",
      "validation data contains 9 points (60.00%) of label 1.\n"
     ]
    }
   ],
   "source": [
    "# update the validation set, so that it doesn't overlap with the training or the previous validation set\n",
    "val_num = 15\n",
    "minus_set = np.append(val_data['smpls'], train_data['smpls'])\n",
    "val_num, val_smpls = update_indices(val_num, ind_min, ind_max, minus_set, seed=None)\n",
    "val_dess, val_lbls = update_set(val_smpls, data_params)\n",
    "val_data   = {'des': val_dess, 'lbls': val_lbls, 'smpls': val_smpls}\n",
    "\n",
    "# show statistics\n",
    "print('new val_num = ', val_num)\n",
    "print('validation data contains {} points ({:.2f}%) of label 1.'\n",
    "      .format(np.sum(val_data['lbls'] == 1), np.sum(val_data['lbls'] == 1)*100/val_num))\n",
    "# print('val_smpls = ', val_data['smpls'])\n",
    "# print('val_lbls = ', val_data['lbls'])\n",
    "# print('val_des = ', val_data['des'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1756ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 474)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['des'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b16a3-2e58-453e-8fa7-63ed55631b2b",
   "metadata": {},
   "source": [
    "## Normalize feature vectors\n",
    "\n",
    "Taken from 'code/03_mnist/sift_on_mnist/sift_on_mnist_06.ipynb' with small changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a14797-9d26-4957-881c-86a599b34831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dess, feature_nrm=1, node_nrm=1):\n",
    "    \"\"\"\n",
    "    Normalize feature vectors.\n",
    "    Inputs: dess, feature_nrm, node_nrm\n",
    "    dess - NxD array of features for all datapoints.\n",
    "    feature_nrm - final norm by feature (columns of dess)\n",
    "    node_nrm - final norm by datapoint/node (rows of dess)\n",
    "    \n",
    "    Outputs: dess_nrm\n",
    "    dess_nrm - NxD array of normalized features.\n",
    "    \"\"\"\n",
    "    # method 2: double normalization\n",
    "    # step 1 - feature-wise: subtract mean and divide by standard deviation of each feature.\n",
    "    dess_mean = np.mean(dess, axis=1, keepdims=True)\n",
    "    dess_std = np.std(dess, axis=1, keepdims=True)\n",
    "\n",
    "    dess_nrm = dess - dess_mean\n",
    "    dess_nrm = dess_nrm * feature_nrm / (dess_std + 0.01)\n",
    "\n",
    "\n",
    "    # step 2 - smaple-wise: normalize l2-norm of each vector to a certain value.\n",
    "    ideal_norm = 30\n",
    "    dess_norm = np.linalg.norm(dess_nrm, axis=0, keepdims=True)\n",
    "    dess_nrm = dess_nrm * node_nrm / (dess_norm + 0.01)\n",
    "    \n",
    "    return dess_nrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceadea69-5252-435f-95fd-cfd2e907f3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train. set features:\n",
      "before:  [[2.8389516 1.8708148 1.8910706 ... 2.7468455 5.0521436 1.7201645]\n",
      " [2.7723725 1.8953145 1.9795811 ... 2.6227593 5.0829363 2.3729057]\n",
      " [2.8612802 1.8065518 1.9427454 ... 1.9574664 4.9966254 1.6729536]\n",
      " ...\n",
      " [2.7531817 1.9815539 1.9276134 ... 2.605453  5.0031734 1.795469 ]\n",
      " [2.804281  1.9247894 1.9292516 ... 2.5805027 5.018913  1.7007477]\n",
      " [2.7910361 1.8826758 2.0089457 ... 2.576486  5.151503  1.7518011]]\n",
      "after:  [[ -9.484925   -9.570793  -10.352366  ...  -8.033644    8.824194\n",
      "   -9.853354 ]\n",
      " [ -9.984946   -9.380919   -9.772409  ...  -8.916292    9.328838\n",
      "   -6.5149717]\n",
      " [ -8.834356   -9.741596   -9.870943  ... -14.435342    8.7642355\n",
      "   -9.933851 ]\n",
      " ...\n",
      " [ -9.780237   -8.767047   -9.905189  ...  -8.756527    9.202212\n",
      "   -9.285356 ]\n",
      " [ -9.202595   -9.112561   -9.940922  ...  -8.988755    9.43847\n",
      "   -9.817366 ]\n",
      " [ -9.705453   -9.573602   -9.715495  ...  -9.318397   10.336171\n",
      "   -9.786439 ]]\n"
     ]
    }
   ],
   "source": [
    "train_dess_nrm = normalize(train_data['des'], feature_nrm=1, node_nrm=30)  \n",
    "val_dess_nrm = normalize(val_data['des'], feature_nrm=1, node_nrm=30)\n",
    "\n",
    "print('train. set features:')\n",
    "print('before: ', train_data['des'])\n",
    "print('after: ', train_dess_nrm)\n",
    "\n",
    "# print('\\nval. set features:')\n",
    "# print('before: ', val_data['des'])\n",
    "# print('after: ', val_dess_nrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ba746-12ad-4e5e-99d2-d93dcd86ffdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40044e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc9956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c654a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
