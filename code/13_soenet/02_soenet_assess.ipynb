{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1559fcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import data_handler_03 as dh\n",
    "import my_simgraph_06 as sg\n",
    "import assess_simgraph_02 as asg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe49625",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa6039-6b29-497a-a224-14527270328f",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45813d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56ec5b-6c16-444b-b66e-9a229113bd24",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd80e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa076e19-d81d-4bb9-9e5f-72a7c5b4bb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25ab2-987b-40b8-bee2-a4b14c982c29",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7368a1d1-d647-4d5c-a8f1-e8586424fbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datapoint_parham(index, data_params):\n",
    "    \"\"\"\n",
    "    Return a single datapoint consisting of (feature vector, label) \n",
    "    based on the extended index system of the whole dataset (297 repeats of a 1141-frame movie); \n",
    "    for example, the 6th frame of the 7th repeat is indexed 7*1141+5. \n",
    "    In this system, indices only move forward after repeats, so they represent time in a sense.\n",
    "    Acceptable index range is batch_sz-1 to 1141*297-1.\n",
    "      \n",
    "    Inputs: index, data_params\n",
    "    index - chosen datapoint's index\n",
    "    data_params   -\n",
    "        func - funtion which returns a datapoint (fv, lbl) based on its index\n",
    "        features_dp - path to where feature vectors are stored\n",
    "        spike_data - (297 x 1141 x m)-shaped array where m is the number of subgroups of neurons.\n",
    "        group_id - index of the chosen subgroup of neurons which is being considered\n",
    "        transform - func. applied to the original feature vector (defult: None, no transform is applied)\n",
    "            \n",
    "    \n",
    "    Output: fv, lbl\n",
    "    fv  - Dfx1 vector representing the selected time bin's feature vector\n",
    "    lbl - the selected time bin's label\n",
    "    \"\"\"\n",
    "    # unpack params\n",
    "    features_dp = data_params['features_dp']\n",
    "    lbl_func = data_params['lbl_func']\n",
    "    transform = data_params['transform'] if 'transform' in data_params else None\n",
    "    \n",
    "    # feature vector\n",
    "    # trial = index//1141\n",
    "    frame = index%1141\n",
    "    fvs = np.load(features_dp)\n",
    "    fv = fvs[frame - 41]\n",
    "    if transform is not None:\n",
    "        fv = transform(fv)\n",
    "\n",
    "    # label  \n",
    "    lbls = lbl_func(data_params)\n",
    "    lbl = lbls[index]\n",
    "    \n",
    "    return fv, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03442d6e-dfae-4b33-bee6-a4b5bc828a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[::2]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "# data retrieval params\n",
    "data_params = {'func': datapoint_parham, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/parham/parham2/features_test_2layer.npy', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': None, 'ind_min': 1*1141+41, 'ind_max': 2*1141-1}\n",
    "\n",
    "# graph construction and penalty term parameters\n",
    "sg_params = {'mu': 39, 'Dt': None, 'Dv':0, 'Dvt':2000, \\\n",
    "             'cnstr_method_tt': 'time', 'cnstr_method_vv': 'time', 'cnstr_method_vt': 'time',\\\n",
    "             'train_t': None, 'val_t': None, \\\n",
    "             'edges_tt':None, 'edges_vv':None, 'edges_vt':None, }\n",
    "\n",
    "# gradient descent parameters\n",
    "sg_opt_params = { 'epsilon0':1, 'epsilon_decay':0.5, 'epsilon_jump': 2, \\\n",
    "                'num_its':16, 'check_freq':1, 'print_checks':False, 'Theta0':None, \\\n",
    "                'force_all_its': True, 'threshold': 0.01}\n",
    "\n",
    "# randomization parameters\n",
    "rnd_params = {'train_sizes': [50, 100, 150, 200, 250, 300], 'val_sizes': [10], 'train_its': 5, 'val_its': 10, 'seed': None}\n",
    "\n",
    "# parameters to visualize the optimized M\n",
    "f_sz = 400 # must match data_params\n",
    "xloc = np.broadcast_to(np.arange(f_sz), (f_sz, f_sz))\n",
    "yloc = xloc.T\n",
    "fig_params = {'rmark_th': 85, 'f_sz': f_sz, 'xloc': xloc, 'yloc': yloc}\n",
    "\n",
    "# path to save the results\n",
    "res_path = '../../data/experiments/parham/parham_sg/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb32f59-072f-4078-924d-8523416c1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_num_res, val_num_err = asg.assess_sg_model(data_params, sg_params, sg_opt_params, rnd_params, fig_params, res_path)\n",
    "asg.plot_curves(rnd_params, sg_params, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11b71c-ed44-4a97-8c84-3c190d97733b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d49e1-79a2-460e-be3a-f2de69323e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56e26-b09b-4d7c-8160-0ab444dde29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a6b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload sg package\n",
    "# import importlib\n",
    "# importlib.reload(asg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65473a02-fcea-4a64-a29f-608f2095ce2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d51c66-3de2-45ed-9a3d-fbdd3f222cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfed75-cd3b-4920-8bdf-36c1b647d962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f234ec-0705-4b99-8f00-a708cc80a4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f23623-a3b1-4d80-bdf2-4e2f2345ebc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed3eb0-dd4c-46ab-9e7a-589cde2f1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668e272-1068-4993-8aca-2fa115e68b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6655fb7-0599-4e9d-99b2-e67e0424be35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
