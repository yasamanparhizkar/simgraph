{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef1d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml # to download mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea632611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(data_set = 'mnist', version='active', data_id=None):\n",
    "    \"\"\"\n",
    "    Download given data_set\n",
    "\n",
    "    Input: data_set, version, data_id\n",
    "    data_set - name of the dataset in OpenML repository\n",
    "    version - version of the dataset\n",
    "    data_id - most exact way of distinguishing a dataset in OpenML repository\n",
    "\n",
    "    Output: raw_data_x, raw_data_y\n",
    "    raw_data_x - NxD\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_id is not None:\n",
    "        raw_data = fetch_openml(data_id = data_id)\n",
    "        \n",
    "    elif data_set == 'mnist':\n",
    "        raw_data = fetch_openml('mnist_784')\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            raw_data = fetch_openml(name = data_set, version = version)\n",
    "        except:\n",
    "            print('Dataset does not exist in OpenML repository.')\n",
    "\n",
    "    # separate numeric data from classification target\n",
    "    raw_data_x = np.array((raw_data['data']+0.5)/256.0)\n",
    "    raw_data_y = np.array(raw_data['target'].astype('int8'))\n",
    "    data_dim = raw_data_x.shape[1]\n",
    "    \n",
    "    return raw_data_x, raw_data_y, data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313ece8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_x, raw_data_y, data_dim = download('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fc734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n",
      "[[0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " ...\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]]\n",
      "[5 0 4 ... 4 5 6]\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_x.shape)\n",
    "print(raw_data_y.shape)\n",
    "print(raw_data_x)\n",
    "print(raw_data_y)\n",
    "print(data_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad80304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(raw_data_x, raw_data_y):\n",
    "    \"\"\"\n",
    "    From any classification dataset, pick out 0 and 1 labeled datapoints and create a new binary classification dataset with them.\n",
    "\n",
    "    Input: raw_data_x, raw_data_y\n",
    "    raw_data_x - NaxD matrix of numeric data. Each row is a datapoint.\n",
    "    raw_data_y - Nax1 vector of corresponding class labels.\n",
    "\n",
    "    Output: data_x, data_y, num_classes\n",
    "    data_x - NbxD matrix of numeric data. Each row is a datapoint.\n",
    "    data_y - Nbx1 vector of corresponding class labels; possible classes are 0 and 1.\n",
    "    num_classes - 2\n",
    "    \"\"\"\n",
    "\n",
    "    accptbl_pts = (raw_data_y == 0) | (raw_data_y == 1)\n",
    "    data_x = raw_data_x[accptbl_pts]\n",
    "    data_y = raw_data_y[accptbl_pts]\n",
    "    num_classes = 2\n",
    "    \n",
    "    return data_x, data_y, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9361907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y, num_classes = binarize(raw_data_x, raw_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a1a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14780, 784)\n",
      "(14780,)\n",
      "[[0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " ...\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]]\n",
      "[0 1 1 ... 1 0 1]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data_x.shape)\n",
    "print(data_y.shape)\n",
    "print(data_x)\n",
    "print(data_y)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d95194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x,y,fracs=[0.8,0.2],seed=0):\n",
    "    \"\"\"\n",
    "    Randomly split data into two sets.\n",
    "\n",
    "    Input: x, y, fracs=[0.8, 0.2], seed=0\n",
    "    x - NxD matrix of x data (e.g. images)\n",
    "    y - Nx1 vector of y data (e.g. labels)\n",
    "    fracs - split fractions determining sizes of set one and set two.\n",
    "    seed - random seed. 'None' disables the use of a new seed.\n",
    "\n",
    "    Output: x1, y1, x2, y2\n",
    "    x1 - (fracs[0]*N)xD matrix of x data of set 1\n",
    "    y1 - (fracs[0]*N)x1 vector of y data of set 1\n",
    "    x2 - (fracs[1]*N)xD matrix of x data of set 2\n",
    "    y2 - (fracs[1]*N)x1 vector of y data of set 2\n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    N = x.shape[0]\n",
    "    rp = np.random.permutation(N)\n",
    "\n",
    "    N1 = int(fracs[0]*N)\n",
    "    N2 = min(N-N1,int(fracs[1]*N))\n",
    "\n",
    "    # split the data into two parts\n",
    "    x1 = x[rp[:N1]]\n",
    "    y1 = y[rp[:N1]]\n",
    "    x2 = x[rp[N1:(N1+N2)]]\n",
    "    y2 = y[rp[N1:(N1+N2)]]\n",
    "\n",
    "    return x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce19f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "portion = 0.01\n",
    "fracs = [0.8,0.2]\n",
    "\n",
    "train_x,train_y,test_x,test_y = split_data(data_x,data_y,fracs=[portion,portion], seed=None)\n",
    "train_x,train_y,  val_x,val_y = split_data(train_x,train_y,fracs=fracs, seed=None)\n",
    "\n",
    "num_train = len(train_y)\n",
    "num_val = len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82ba4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 784)\n",
      "(117,)\n",
      "117\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(num_train)\n",
    "print(num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daffee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2D_U8(data_x, img_sz): \n",
    "    \"\"\"\n",
    "    Grayscale images are sometimes represented as 1D vectors of [0,1] fractional entries.\n",
    "    This function converts the representation to 2D matrices of [0,255] integer entries.\n",
    "\n",
    "    Input: data_x, img_sz\n",
    "    data_x - NxD matrix of all images in the dataset. Each row is one image.\n",
    "    img_sz - (length, width) of the images in pixels (e.g. (28,28))\n",
    "\n",
    "    Output: cnv_data_x\n",
    "    cnv_data_x - (N x length x width) tensor of all images in the dataset with the new representation.\n",
    "    \"\"\"\n",
    "\n",
    "    N = data_x.shape[0]\n",
    "    length = img_sz[0]\n",
    "    width = img_sz[1]\n",
    "\n",
    "    cnv_data_x = data_x.reshape((N, length, width))\n",
    "    cnv_data_x = (cnv_data_x*256).astype('uint8')\n",
    "\n",
    "    return cnv_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4e8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz = (28,28)\n",
    "cnv_train_x = convert_2D_U8(train_x, img_sz)\n",
    "cnv_val_x = convert_2D_U8(val_x, img_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75584e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 28, 28)\n",
      "(29, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(cnv_train_x.shape)\n",
    "print(cnv_val_x.shape)\n",
    "print(cnv_train_x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f6a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift(images, sift_params):\n",
    "    \"\"\"\n",
    "    Extracts SIFT keypoints and descriptors for all input images.\n",
    "\n",
    "    Input: images, sift_params\n",
    "    images - (N x length x width) tensor of N grayscale images with size (length, width) and integer entries between [0,255]\n",
    "    sift_params - parameters of the SIFT algorithm\n",
    "\n",
    "    Output: kp, des\n",
    "    kp - \n",
    "    des -\n",
    "    \"\"\"\n",
    "\n",
    "    # define SIFT parameters. If no parameters are given, resort to default values.\n",
    "    if sift_params is None:\n",
    "        nfeatures = 0\n",
    "        nOctaveLayers = 3\n",
    "        contrastThreshold = 0.04\n",
    "        edgeThreshold = 10\n",
    "        sigma = 1.6\n",
    "    else:\n",
    "        nfeatures = sift_params['nfeatures']\n",
    "        nOctaveLayers = sift_params['nOctaveLayers']\n",
    "        contrastThreshold = sift_params['contrastThreshold']\n",
    "        edgeThreshold = sift_params['edgeThreshold']\n",
    "        sigma = sift_params['sigma']\n",
    "        \n",
    "    # create SIFT object with given parameters\n",
    "    sift = cv.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)\n",
    "    \n",
    "    # extract SIFT keypoints and descriptors for all images\n",
    "    N = images.shape[0]\n",
    "    kp = []\n",
    "    des = []\n",
    "    for i in range(N):\n",
    "        _kp, _des = sift.detectAndCompute(images[i],None)\n",
    "        kp.append(_kp)\n",
    "        des.append(_des)\n",
    "\n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "937547f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift_params = {'nfeatures': 0, \\\n",
    "               'nOctaveLayers':3, \\\n",
    "               'contrastThreshold':0.04, \\\n",
    "               'edgeThreshold':10, \\\n",
    "               'sigma':1.6}\n",
    "kp, des = sift(cnv_train_x, sift_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c898578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "117\n",
      "(10, 128)\n",
      "(<KeyPoint 0x7fb8445c08a0>, <KeyPoint 0x7fb8445c0960>, <KeyPoint 0x7fb8445c0570>, <KeyPoint 0x7fb8445c0990>, <KeyPoint 0x7fb8445c04e0>, <KeyPoint 0x7fb8445c06c0>, <KeyPoint 0x7fb8445c0750>, <KeyPoint 0x7fb7d1845ea0>, <KeyPoint 0x7fb7d1845e70>, <KeyPoint 0x7fb7d1845f60>)\n",
      "[[ 13.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  2.   0.   1. ...   0.   0.   1.]\n",
      " ...\n",
      " [ 11.  37.  66. ...   0.   0.   0.]\n",
      " [  6.   0.   0. ...  30.  56. 102.]\n",
      " [ 20.   0.   0. ...   1.   5.   1.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(kp))\n",
    "print(len(des))\n",
    "print(des[0].shape)\n",
    "print(kp[0])\n",
    "print(des[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5777fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpnum_floor(kp, des, cutoff):\n",
    "    \"\"\"\n",
    "    Receive SIFT keypoints and descriptors of a number of images. Discard all images with less than a certain number of keypoints.\n",
    "\n",
    "    Input: kp, des, cutoff\n",
    "    kp - list of tuples of keypoints of images.\n",
    "    des - list of nx128 matrices of descriptors of corresponding images.\n",
    "    cutoff - minimum acceptable number of keypoints in each image.\n",
    "\n",
    "    Output: ct_kp, ct_des, ct_ind\n",
    "    ct_kp - list of tuples of keypoints of images with more than the minimum number of keypoints.\n",
    "    ct_des - list of nx128 matrices of descriptors of corresponding images.\n",
    "    ct_ind - indices of images with more than the minimum number of keypoints in the original dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(kp)\n",
    "    ct_ind = [i for i in range(N) if len(kp[i])>=cutoff]\n",
    "    ct_kp = [kp[i] for i in ct_ind]\n",
    "    ct_des = [des[i] for i in ct_ind]\n",
    "\n",
    "    return ct_kp, ct_des, ct_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98255112",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_kp, ct_des, ct_ind = kpnum_floor(kp, des, 2)\n",
    "num_train = len(ct_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c085891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "106\n",
      "106\n",
      "[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116]\n",
      "(<KeyPoint 0x7fb7d10d50f0>, <KeyPoint 0x7fb7d10d5330>, <KeyPoint 0x7fb7d10d51b0>, <KeyPoint 0x7fb7d10d51e0>, <KeyPoint 0x7fb7d10d5210>, <KeyPoint 0x7fb7d10d5090>, <KeyPoint 0x7fb7d10d5180>, <KeyPoint 0x7fb7d10d5240>, <KeyPoint 0x7fb7d10d5120>, <KeyPoint 0x7fb7d10d5150>)\n",
      "(10, 128)\n"
     ]
    }
   ],
   "source": [
    "print(len(ct_kp))\n",
    "print(len(ct_des))\n",
    "print(len(ct_ind))\n",
    "print(ct_ind)\n",
    "print(ct_kp[1])\n",
    "print(ct_des[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "39784134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpnum_ceil(kp, des, cutoff):\n",
    "    \"\"\"\n",
    "    Receive SIFT keypoints and descriptors of a number of images.\n",
    "    For images with more than 'cutoff' number of keypoints, keep only 'cutoff' keypoints based on their proximity to the top-left corner.\n",
    "    For example, if cutoff is 2, keep only 1 keypoint closest to and 1 keypoint farthest from the top-left corner.\n",
    "    This strategy hopefully leads to the widest possible coverage of the image by the keypoints' descriptors.\n",
    "    -----\n",
    "    IMPORTANT Note: If you are using this function alongside kpnum_floor, you should use kpnum_floor before this function.\n",
    "\n",
    "    Input: kp, des, cutoff\n",
    "    kp - list of tuples of keypoints of images.\n",
    "    des - list of nx128 matrices of descriptors of corresponding images.\n",
    "    cutoff - maximum acceptable number of keypoints in each image.\n",
    "\n",
    "    Output: ct_kp, ct_des\n",
    "    ct_kp - list of tuples of keypoints of images, now having fewer equal keypoints to the given maximum.\n",
    "    ct_des - list of (n*128)x1 vectors of concatenated descriptors of all keypoints of corresponding images.\n",
    "    \"\"\"\n",
    "\n",
    "    assert cutoff > 0\n",
    "\n",
    "    ct_kp = []\n",
    "    ct_des = []\n",
    "    N = len(kp)\n",
    "\n",
    "    if cutoff == 2:\n",
    "        for i in range(N):\n",
    "            # calculate keypoints' distances from the top-left corner\n",
    "            dists = [np.sum(np.array(key.pt)**2) for key in kp[i]]\n",
    "\n",
    "            # choose keypoints closest and farthest from the top-left corner\n",
    "            left_pt = np.argmin(dists)\n",
    "            right_pt = np.argmax(dists)\n",
    "\n",
    "            # create new lists with chosen keypoints and the concatenation of their descriptors\n",
    "            kept_kps = (kp[i][left_pt], kp[i][right_pt])\n",
    "            cat_des = np.append(des[i][left_pt], des[i][right_pt], axis=0)\n",
    "            ct_kp.append(kept_kps)\n",
    "            ct_des.append(cat_des)\n",
    "\n",
    "    else:\n",
    "        nr = int(cutoff/2) # number of keypoints farthest from the top-left corner\n",
    "        nl = cutoff - nr   # number of keypoints closest to the top-left corner\n",
    "\n",
    "        for i in range(N):\n",
    "            # calculate keypoints' distances from the top-left corner\n",
    "            n = len(kp[i])\n",
    "            dists = [(j, np.sum(np.array(kp[i][j].pt)**2)) for j in range(n)]\n",
    "\n",
    "            # sort keypoints by their distances\n",
    "            sorted_dists = sorted(dists, key=lambda x:x[1])\n",
    "\n",
    "            # keep the right number of keypoints closes to and farthest from the top-left corner\n",
    "            raw_ind = [j if j < nl else -1*(j-nl+1) for j in range(cutoff)]\n",
    "            kp_ind = [sorted_dists[ind][0] for ind in raw_ind]\n",
    "\n",
    "            kept_kps = tuple([kp[i][ind] for ind in kp_ind])\n",
    "            cat_des = des[i][kp_ind,:].reshape(-1)\n",
    "\n",
    "            ct_kp.append(kept_kps)\n",
    "            ct_des.append(cat_des)\n",
    "\n",
    "    return ct_kp, ct_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e87169dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cct_kp, cct_des = kpnum_ceil(ct_kp, ct_des, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "973d731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(cct_kp))\n",
    "print(type(cct_des))\n",
    "for i in range(len(cct_kp)):\n",
    "    if len(cct_kp[i]) != 2 or cct_des[i].shape[0] != 256:\n",
    "        print(\"Trouble!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fec9328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(descriptors):\n",
    "    \"\"\"\n",
    "    Receive feature vectors describing a number of images (e.g. concatenated SIFT descriptors). Normalize the vectors according to below algorithm.\n",
    "    There are two steps in this normalization: feature-wise and sample-wise (aka. image-wise).\n",
    "        1. feature-wise: subtract mean and divide by standard deviation of each feature for all images.\n",
    "        2. sample-wise: normalize l2-norm of each vector to 1.\n",
    "\n",
    "    Input: descriptors\n",
    "    descriptors - NxDf matrix of vectors describing N images.\n",
    "\n",
    "    Output: nrm_dess\n",
    "    nrm_dess - NxDf matrix of normalized vectors describing N images.\n",
    "    \"\"\"\n",
    "\n",
    "    # feature-wise normalization\n",
    "    means = np.mean(descriptors, axis=0, keepdims=True)\n",
    "    stds = np.std(descriptors, axis=0, keepdims=True)\n",
    "    nrm_dess = descriptors - means\n",
    "    nrm_dess = nrm_dess / (stds + 0.01)\n",
    "\n",
    "    # sample-wise normalization\n",
    "    norms = np.linalg.norm(nrm_dess, axis=1, keepdims=True)\n",
    "    nrm_dess = nrm_dess / (norms + 0.01)\n",
    "\n",
    "    return nrm_dess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "13ff2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrm_des = normalize(np.array(cct_des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3b2cb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 256)\n"
     ]
    }
   ],
   "source": [
    "print(nrm_des.shape)\n",
    "for i in range(nrm_des.shape[0]):\n",
    "    if np.abs(np.linalg.norm(nrm_des[i]) - 1) > 0.01:\n",
    "        print(\"Trouble in mean #{} : {}\".format(i, np.mean(nrm_des[i])))\n",
    "        \n",
    "assert (np.abs(np.mean(nrm_des, axis=0, keepdims=True)) < 0.1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1da7625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(nrm_des) == np.ndarray or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f79c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b734e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f65565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227850e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
