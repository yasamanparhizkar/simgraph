{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a53fec-ff77-405e-a789-8009292a1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/simgraph/code/my_packages')\n",
    "sys.path.append('F:/Users/yasam/Documents/GitHub/simgraph/code/my_packages')\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import dataprocess.data_handler_03 as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d20be-b151-4b06-9341-1e33f2658890",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial source:\n",
    "https://www.youtube.com/watch?v=0_PgWWmauHk\n",
    "\n",
    "Load 3D-SIFT features and corresponding labels instead of MNIST data in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8528de-8d62-40ac-b2e9-96f9db95429a",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38d2495-d40d-40e0-97ea-642da02f9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../../local_data/original_files/spikes.npy'\n",
    "binned_data = np.load(spikes_dp)\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]\n",
    "\n",
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abcc976-477c-446b-90c0-b570c70766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_labels(data_params):\n",
    "    return np.loadtxt(data_params['features_dp']+'lbls.csv')\n",
    "\n",
    "def transform_mnistsift(fv):\n",
    "    return fv[::4]\n",
    "\n",
    "def transform_slowfast(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[::100]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "def transform_sift3d(fv):\n",
    "    return fv[::2]\n",
    "\n",
    "def transform_parham(fv):\n",
    "    return fv[::5]\n",
    "\n",
    "# data retrieval params\n",
    "# data_params = {'func': dh.datapoint_sift, 'lbl_func': get_mnist_labels, 'features_dp': '../../data/fe_exp/mnist-sift/', \\\n",
    "#                'spike_data': None, 'group_id': None, 'transform': transform_mnistsift, 'ind_min': 0, 'ind_max': 13203, 'feature_id':'mnist-sift'}\n",
    "\n",
    "# data_params = {'func': dh.datapoint_numpy, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/slowfast/slowfast_4732_numpy/', \\\n",
    "#                'spike_data': grouped_data, 'group_id': 0, 'transform': transform_slowfast, 'ind_min': 1*1141+0, 'ind_max': 2*1141-1, 'feature_id':'slowfast'}\n",
    "\n",
    "data_params = {'func': dh.datapoint_numpy, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/sift3d/fvs_s1_with_kp/desc/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform_sift3d, 'ind_min': 1*1141+0, 'ind_max': 2*1141-1, 'feature_id':'sift3d'}\n",
    "\n",
    "# data_params = {'func': dh.datapoint_numpy, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/parham/parham3/features_2layer/', \\\n",
    "#                'spike_data': grouped_data, 'group_id': 0, 'transform': transform_parham, 'ind_min': 1*1141+41, 'ind_max': 2*1141-1, 'feature_id':'parham'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43739e7e-b846-4d76-b964-4c120b6fd5c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_id:  sift3d\n",
      "train_num =  100 , val_num =  50\n",
      "number of features:  384\n",
      "training data contains 50 points (50.00%) of label 1.\n",
      "validation data contains 25 points (50.00%) of label 1.\n",
      "train_smpls =  [2119 1320 1379 1527 1868 1362 1482 1775 2194 1638 1510 1752 1948 1716\n",
      " 1845 1278 1426 2135 1388 1647 1899 1913 1169 2182 1795 1717 1931 1300\n",
      " 2132 2184 2170 1460 1457 2152 1759 1316 2149 1599 1223 2155 1671 2180\n",
      " 1824 1826 1268 1949 1344 1684 2257 2221 1289 2249 1246 2074 2076 1226\n",
      " 1551 1574 1340 1217 1374 1959 2231 2280 1976 1236 1145 1190 1181 1991\n",
      " 1238 1176 2276 2013 1191 1184 1581 1989 1972 2210 1177 1242 2095 2068\n",
      " 1573 2077 1215 1182 2226 1153 1990 1601 1173 1561 2059 1544 2071 1552\n",
      " 2022 2079] \n",
      "val_smpls =  [2175 2208 1303 2150 1432 1394 1947 2207 1882 1728 1465 1846 1771 1967\n",
      " 1468 1321 1969 1727 1535 2137 1809 2195 1489 1609 1369 1141 2094 2089\n",
      " 2052 1225 1262 1582 1624 2240 1975 2045 1550 1559 1540 1549 1858 2069\n",
      " 1977 1201 1585 1545 1852 1165 2054 1229]\n",
      "train_lbls =  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] \n",
      "val_lbls =  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "train_des =  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 1.4917e-02 2.5268e-02 2.0445e-02]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 1.7730e-03 3.5250e-03 1.3330e-03]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 3.5000e-05 4.8000e-05 1.3100e-04]\n",
      " ...\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 7.5000e-05 4.4000e-05 2.4000e-05]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 2.3761e-02 1.2640e-02 2.4714e-02]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 2.3415e-02 3.9590e-02 3.1565e-02]] \n",
      "val_des =  [[0.0000e+00 0.0000e+00 0.0000e+00 ... 6.4000e-05 1.4800e-04 2.1100e-04]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 9.6697e-02 9.6697e-02 5.2499e-02]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 5.0000e-06 1.9000e-05 3.0000e-05]\n",
      " ...\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 2.6014e-02 4.2962e-02 3.7094e-02]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 9.7780e-03 1.5369e-02 2.1704e-02]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 7.3790e-03 1.0948e-02 8.0160e-03]]\n"
     ]
    }
   ],
   "source": [
    "train_num = 10\n",
    "val_num = 5\n",
    "\n",
    "train_num, val_num, train_data, val_data = \\\n",
    "dh.random_train_val_balanced(train_num, val_num, data_params, seed=1342)\n",
    "\n",
    "# show statistics\n",
    "print('feature_id: ', data_params['feature_id'])\n",
    "print('train_num = ', train_num, ', val_num = ', val_num)\n",
    "print('number of features: ', train_data['des'].shape[1])\n",
    "print('training data contains {} points ({:.2f}%) of label 1.'\n",
    "      .format(np.sum(train_data['lbls'] == 1), np.sum(train_data['lbls'] == 1)*100/train_num))\n",
    "print('validation data contains {} points ({:.2f}%) of label 1.'\n",
    "      .format(np.sum(val_data['lbls'] == 1), np.sum(val_data['lbls'] == 1)*100/val_num))\n",
    "print('train_smpls = ', train_data['smpls'], '\\nval_smpls = ', val_data['smpls'])\n",
    "print('train_lbls = ', train_data['lbls'], '\\nval_lbls = ', val_data['lbls'])\n",
    "print('train_des = ', train_data['des'], '\\nval_des = ', val_data['des'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0de8aa-55d5-45a8-9445-d69cd1c56d8f",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e1db42b0-aa0f-4b01-803b-5ec6137b5b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 2\n",
    "num_epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# to resemble the graph optimization better, we only have one large batch.\n",
    "batch_size_train = train_num\n",
    "batch_size_val = val_num\n",
    "\n",
    "input_size = 12\n",
    "sequence_length = 32\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44481a9c-672f-4096-96d2-a4192d4a6a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe997a-099e-4415-8100-5e8fa8aff7e9",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4864b2a3-11dc-4ecb-83cd-ce31afaedad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 100\n",
    "val_num = 50\n",
    "\n",
    "train_num, val_num, train_data, val_data = \\\n",
    "dh.random_train_val_balanced(train_num, val_num, data_params, seed=1342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "43b8fd32-36d1-4679-842a-f2acf1acba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 32, 12)\n",
      "(1, 100)\n",
      "(1, 50, 32, 12)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "train_des = train_data['des'].reshape(-1, batch_size_train, sequence_length, input_size)\n",
    "train_lbls = train_data['lbls'].reshape(-1, batch_size_train)\n",
    "train_lbls = (train_lbls + 1)//2\n",
    "print(train_des.shape)\n",
    "print(train_lbls.shape)\n",
    "\n",
    "val_des = val_data['des'].reshape(-1, batch_size_val, sequence_length, input_size)\n",
    "val_lbls = val_data['lbls'].reshape(-1, batch_size_val)\n",
    "val_lbls = (val_lbls + 1)//2\n",
    "print(val_des.shape)\n",
    "print(val_lbls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfd5fef5-ce02-4f0e-a926-7f690d41f3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 , 1\n"
     ]
    }
   ],
   "source": [
    "# number of all batches in the training data\n",
    "n_total_steps = train_num // batch_size_train\n",
    "n_val_batches = val_num // batch_size_val\n",
    "\n",
    "print(n_total_steps, ',', n_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "23a1cb41-ccdb-4ad9-9098-6818f0da0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datapoint(des, lbls, step_i):\n",
    "    features = torch.from_numpy(des[step_i, :, :, :]).to(torch.float32)\n",
    "    labels = torch.from_numpy(lbls[step_i, :]).to(torch.int64)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc27a1-260d-4c75-93a9-cf2a99fce084",
   "metadata": {},
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7f454257-9669-4b5e-a315-45931b6483e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "70915658-9121-4f40-9ab0-5add38989916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8773e0d3-9a4a-46ca-8b6b-67ef634a1718",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/1], Loss: 0.6928\n",
      "Epoch [2/100], Step [1/1], Loss: 0.6970\n",
      "Epoch [3/100], Step [1/1], Loss: 0.6916\n",
      "Epoch [4/100], Step [1/1], Loss: 0.6913\n",
      "Epoch [5/100], Step [1/1], Loss: 0.6916\n",
      "Epoch [6/100], Step [1/1], Loss: 0.6902\n",
      "Epoch [7/100], Step [1/1], Loss: 0.6884\n",
      "Epoch [8/100], Step [1/1], Loss: 0.6869\n",
      "Epoch [9/100], Step [1/1], Loss: 0.6858\n",
      "Epoch [10/100], Step [1/1], Loss: 0.6842\n",
      "Epoch [11/100], Step [1/1], Loss: 0.6816\n",
      "Epoch [12/100], Step [1/1], Loss: 0.6775\n",
      "Epoch [13/100], Step [1/1], Loss: 0.6710\n",
      "Epoch [14/100], Step [1/1], Loss: 0.6614\n",
      "Epoch [15/100], Step [1/1], Loss: 0.6898\n",
      "Epoch [16/100], Step [1/1], Loss: 0.6575\n",
      "Epoch [17/100], Step [1/1], Loss: 0.6566\n",
      "Epoch [18/100], Step [1/1], Loss: 0.6598\n",
      "Epoch [19/100], Step [1/1], Loss: 0.6616\n",
      "Epoch [20/100], Step [1/1], Loss: 0.6621\n",
      "Epoch [21/100], Step [1/1], Loss: 0.6616\n",
      "Epoch [22/100], Step [1/1], Loss: 0.6603\n",
      "Epoch [23/100], Step [1/1], Loss: 0.6584\n",
      "Epoch [24/100], Step [1/1], Loss: 0.6558\n",
      "Epoch [25/100], Step [1/1], Loss: 0.6526\n",
      "Epoch [26/100], Step [1/1], Loss: 0.6485\n",
      "Epoch [27/100], Step [1/1], Loss: 0.6434\n",
      "Epoch [28/100], Step [1/1], Loss: 0.6370\n",
      "Epoch [29/100], Step [1/1], Loss: 0.6302\n",
      "Epoch [30/100], Step [1/1], Loss: 0.6275\n",
      "Epoch [31/100], Step [1/1], Loss: 0.6276\n",
      "Epoch [32/100], Step [1/1], Loss: 0.6186\n",
      "Epoch [33/100], Step [1/1], Loss: 0.6150\n",
      "Epoch [34/100], Step [1/1], Loss: 0.6130\n",
      "Epoch [35/100], Step [1/1], Loss: 0.6055\n",
      "Epoch [36/100], Step [1/1], Loss: 0.5960\n",
      "Epoch [37/100], Step [1/1], Loss: 0.5931\n",
      "Epoch [38/100], Step [1/1], Loss: 0.5884\n",
      "Epoch [39/100], Step [1/1], Loss: 0.5767\n",
      "Epoch [40/100], Step [1/1], Loss: 0.5684\n",
      "Epoch [41/100], Step [1/1], Loss: 0.5600\n",
      "Epoch [42/100], Step [1/1], Loss: 0.5532\n",
      "Epoch [43/100], Step [1/1], Loss: 0.5372\n",
      "Epoch [44/100], Step [1/1], Loss: 0.5407\n",
      "Epoch [45/100], Step [1/1], Loss: 0.5309\n",
      "Epoch [46/100], Step [1/1], Loss: 0.5314\n",
      "Epoch [47/100], Step [1/1], Loss: 0.5551\n",
      "Epoch [48/100], Step [1/1], Loss: 0.5147\n",
      "Epoch [49/100], Step [1/1], Loss: 0.5153\n",
      "Epoch [50/100], Step [1/1], Loss: 0.5179\n",
      "Epoch [51/100], Step [1/1], Loss: 0.5077\n",
      "Epoch [52/100], Step [1/1], Loss: 0.5090\n",
      "Epoch [53/100], Step [1/1], Loss: 0.5084\n",
      "Epoch [54/100], Step [1/1], Loss: 0.4988\n",
      "Epoch [55/100], Step [1/1], Loss: 0.4983\n",
      "Epoch [56/100], Step [1/1], Loss: 0.4947\n",
      "Epoch [57/100], Step [1/1], Loss: 0.4881\n",
      "Epoch [58/100], Step [1/1], Loss: 0.4858\n",
      "Epoch [59/100], Step [1/1], Loss: 0.4914\n",
      "Epoch [60/100], Step [1/1], Loss: 0.5101\n",
      "Epoch [61/100], Step [1/1], Loss: 0.5345\n",
      "Epoch [62/100], Step [1/1], Loss: 0.4698\n",
      "Epoch [63/100], Step [1/1], Loss: 0.4913\n",
      "Epoch [64/100], Step [1/1], Loss: 0.4870\n",
      "Epoch [65/100], Step [1/1], Loss: 0.4836\n",
      "Epoch [66/100], Step [1/1], Loss: 0.4795\n",
      "Epoch [67/100], Step [1/1], Loss: 0.4670\n",
      "Epoch [68/100], Step [1/1], Loss: 0.4841\n",
      "Epoch [69/100], Step [1/1], Loss: 0.4579\n",
      "Epoch [70/100], Step [1/1], Loss: 0.4701\n",
      "Epoch [71/100], Step [1/1], Loss: 0.4486\n",
      "Epoch [72/100], Step [1/1], Loss: 0.4658\n",
      "Epoch [73/100], Step [1/1], Loss: 0.4421\n",
      "Epoch [74/100], Step [1/1], Loss: 0.4498\n",
      "Epoch [75/100], Step [1/1], Loss: 0.4345\n",
      "Epoch [76/100], Step [1/1], Loss: 0.4366\n",
      "Epoch [77/100], Step [1/1], Loss: 0.4349\n",
      "Epoch [78/100], Step [1/1], Loss: 0.4268\n",
      "Epoch [79/100], Step [1/1], Loss: 0.4288\n",
      "Epoch [80/100], Step [1/1], Loss: 0.4131\n",
      "Epoch [81/100], Step [1/1], Loss: 0.4197\n",
      "Epoch [82/100], Step [1/1], Loss: 0.4127\n",
      "Epoch [83/100], Step [1/1], Loss: 0.4048\n",
      "Epoch [84/100], Step [1/1], Loss: 0.3983\n",
      "Epoch [85/100], Step [1/1], Loss: 0.3999\n",
      "Epoch [86/100], Step [1/1], Loss: 0.3902\n",
      "Epoch [87/100], Step [1/1], Loss: 0.3912\n",
      "Epoch [88/100], Step [1/1], Loss: 0.3836\n",
      "Epoch [89/100], Step [1/1], Loss: 0.3819\n",
      "Epoch [90/100], Step [1/1], Loss: 0.3777\n",
      "Epoch [91/100], Step [1/1], Loss: 0.3755\n",
      "Epoch [92/100], Step [1/1], Loss: 0.3728\n",
      "Epoch [93/100], Step [1/1], Loss: 0.3638\n",
      "Epoch [94/100], Step [1/1], Loss: 0.3642\n",
      "Epoch [95/100], Step [1/1], Loss: 0.3617\n",
      "Epoch [96/100], Step [1/1], Loss: 0.3675\n",
      "Epoch [97/100], Step [1/1], Loss: 0.4329\n",
      "Epoch [98/100], Step [1/1], Loss: 0.4362\n",
      "Epoch [99/100], Step [1/1], Loss: 0.3583\n",
      "Epoch [100/100], Step [1/1], Loss: 0.3974\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(n_total_steps):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        images, labels = get_datapoint(train_des, train_lbls, i)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5352165c-52da-4f2a-a7c5-c79ca2f625fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74.0 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i in range(n_val_batches): # take care not to repeat training data\n",
    "        images, labels = get_datapoint(val_des, val_lbls, i)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57871c10-bce9-44d8-a158-9ec28befd228",
   "metadata": {},
   "source": [
    "# Make functions to train and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37330468-f961-4820-ba66-697c38935817",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca3a3a62-7d8f-467b-a798-22f2e4dc5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(input_size, hidden_size, num_layers, num_classes, device, learning_rate, \n",
    "              num_epochs, n_total_steps, train_des, train_lbls):\n",
    "    model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(n_total_steps):  \n",
    "            # origin shape: [N, 1, 28, 28]\n",
    "            # resized: [N, 28, 28]\n",
    "            images, labels = get_datapoint(train_des, train_lbls, i)\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 1 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13977f35-503a-47af-ab8c-834ef63e072c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1/1], Loss: 0.6931\n",
      "Epoch [2/50], Step [1/1], Loss: 0.6950\n",
      "Epoch [3/50], Step [1/1], Loss: 0.6917\n",
      "Epoch [4/50], Step [1/1], Loss: 0.6928\n",
      "Epoch [5/50], Step [1/1], Loss: 0.6911\n",
      "Epoch [6/50], Step [1/1], Loss: 0.6896\n",
      "Epoch [7/50], Step [1/1], Loss: 0.6890\n",
      "Epoch [8/50], Step [1/1], Loss: 0.6886\n",
      "Epoch [9/50], Step [1/1], Loss: 0.6875\n",
      "Epoch [10/50], Step [1/1], Loss: 0.6858\n",
      "Epoch [11/50], Step [1/1], Loss: 0.6839\n",
      "Epoch [12/50], Step [1/1], Loss: 0.6817\n",
      "Epoch [13/50], Step [1/1], Loss: 0.6787\n",
      "Epoch [14/50], Step [1/1], Loss: 0.6732\n",
      "Epoch [15/50], Step [1/1], Loss: 0.6625\n",
      "Epoch [16/50], Step [1/1], Loss: 0.6477\n",
      "Epoch [17/50], Step [1/1], Loss: 0.6904\n",
      "Epoch [18/50], Step [1/1], Loss: 0.6458\n",
      "Epoch [19/50], Step [1/1], Loss: 0.6454\n",
      "Epoch [20/50], Step [1/1], Loss: 0.6500\n",
      "Epoch [21/50], Step [1/1], Loss: 0.6536\n",
      "Epoch [22/50], Step [1/1], Loss: 0.6557\n",
      "Epoch [23/50], Step [1/1], Loss: 0.6567\n",
      "Epoch [24/50], Step [1/1], Loss: 0.6567\n",
      "Epoch [25/50], Step [1/1], Loss: 0.6558\n",
      "Epoch [26/50], Step [1/1], Loss: 0.6539\n",
      "Epoch [27/50], Step [1/1], Loss: 0.6509\n",
      "Epoch [28/50], Step [1/1], Loss: 0.6468\n",
      "Epoch [29/50], Step [1/1], Loss: 0.6416\n",
      "Epoch [30/50], Step [1/1], Loss: 0.6352\n",
      "Epoch [31/50], Step [1/1], Loss: 0.6276\n",
      "Epoch [32/50], Step [1/1], Loss: 0.6196\n",
      "Epoch [33/50], Step [1/1], Loss: 0.6132\n",
      "Epoch [34/50], Step [1/1], Loss: 0.6057\n",
      "Epoch [35/50], Step [1/1], Loss: 0.5991\n",
      "Epoch [36/50], Step [1/1], Loss: 0.6026\n",
      "Epoch [37/50], Step [1/1], Loss: 0.5985\n",
      "Epoch [38/50], Step [1/1], Loss: 0.5950\n",
      "Epoch [39/50], Step [1/1], Loss: 0.5871\n",
      "Epoch [40/50], Step [1/1], Loss: 0.5819\n",
      "Epoch [41/50], Step [1/1], Loss: 0.5812\n",
      "Epoch [42/50], Step [1/1], Loss: 0.5803\n",
      "Epoch [43/50], Step [1/1], Loss: 0.5790\n",
      "Epoch [44/50], Step [1/1], Loss: 0.5727\n",
      "Epoch [45/50], Step [1/1], Loss: 0.5661\n",
      "Epoch [46/50], Step [1/1], Loss: 0.5600\n",
      "Epoch [47/50], Step [1/1], Loss: 0.5552\n",
      "Epoch [48/50], Step [1/1], Loss: 0.5533\n",
      "Epoch [49/50], Step [1/1], Loss: 0.5493\n",
      "Epoch [50/50], Step [1/1], Loss: 0.5439\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn(input_size, hidden_size, num_layers, num_classes, device, learning_rate, \n",
    "              num_epochs, n_total_steps, train_des, train_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478ed10-1c33-4639-8267-9dbdfb6d539d",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e337758-f184-4af2-bafd-748686aa9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn(model, n_val_batches, val_des, val_lbls):\n",
    "    # Test the model\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(n_val_batches): # take care not to repeat training data\n",
    "            images, labels = get_datapoint(val_des, val_lbls, i)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4404638-40fb-4066-a418-811df5d9bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 78.0 %\n"
     ]
    }
   ],
   "source": [
    "acc = test_rnn(model, n_val_batches, val_des, val_lbls)\n",
    "print(f'Accuracy of the network on the test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140e06f-7255-4f00-90a5-b66dda45b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003582a8-91bc-4828-babd-a7548aa4e24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242f773-7461-4a44-95b9-f8d5d11a34b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4fa9b-4974-4ed8-8872-45d16b449a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aeee77-6419-4256-826d-33491f7ab258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18d24e-7547-4390-bfc7-feb5aede5dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78549839-2992-4920-ade3-1fed0bff596a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443e4d0-ba28-4449-a341-5d6f555e0757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f5cd8-731e-49a8-a067-c9c5de5c4b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05af839-69a0-4615-9812-158322af8861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5368e86-2a9c-429e-b2ac-e8cfd6b97b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44098b-ef54-4241-857a-91e67c1e0b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68dc790-1f43-432e-9c47-06ff19c9d2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345ef96-ab2f-49d5-b433-57556a6ebde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570789a0-4902-4dd7-a70e-3f821f77dd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "74f9c2e0-689d-41d4-9141-0daacb55ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a63f8715-64c5-4048-85ba-a9c851c58cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef98e7b-3ae3-4cc9-8c2d-faf42950714a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d121c-4a81-418c-9efb-227d0a124056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d1db4-b16b-458e-9f70-38adf8b92955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77602408-8829-44ad-8e7a-4c1383e9b6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeade75c-bbd9-49dc-8964-59bfab008528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe032ca-4b82-44a4-8b91-35b44aa7550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44287c0-cd70-45bf-b6ec-f07e425884f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248c419-f3ae-4163-84c0-f82ce52f6991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
