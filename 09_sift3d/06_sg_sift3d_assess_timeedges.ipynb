{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1559fcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# add the path to my packages to system paths so they can be imported\n",
    "import sys\n",
    "# sys.path.append('/home/yasamanparhizkar/Documents/yorku/01_thesis/code/my_packages')\n",
    "# sys.path.append('F:\\MAScThesis\\code\\my_packages')\n",
    "sys.path.append('/home/yasamanparhizkar/Documents/thesis/code/my_packages')\n",
    "\n",
    "import data_handler_03 as dh\n",
    "import my_simgraph_06 as sg\n",
    "import assess_simgraph_02 as asg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe49625",
   "metadata": {},
   "source": [
    "# Load spike data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa6039-6b29-497a-a224-14527270328f",
   "metadata": {},
   "source": [
    "Spike data shape:  (297, 1141, 113) $\\implies$ (movie repeats, frames/time, neurons)\n",
    "<br>\n",
    "Labels are 1 (= spike) or -1 (= no spike)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45813d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all spike data from file\n",
    "spikes_dp = '../../data/original_files/spikes.csv'\n",
    "binned_data = np.loadtxt(spikes_dp, delimiter=',')\n",
    "binned_data = binned_data.reshape(binned_data.shape[0], 1141, 113)\n",
    "binned_data = binned_data * 2 - 1     # turn labels from 0,1 to -1,1\n",
    "\n",
    "I_order_10 = [54, 35, 10, 60, 74, 9, 61, 56, 91, 104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56ec5b-6c16-444b-b66e-9a229113bd24",
   "metadata": {},
   "source": [
    "## Group all 113 neurons\n",
    "\n",
    "This will create a more balanced dataset which is presumabley easier to solve.\n",
    "<br>\n",
    "Grouped data shape:  (297, 1141, 1) $\\implies$ (movie repeats, frames/time, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98bd80e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group all neurons together\n",
    "grouped_data = np.zeros((297, 1141, 1))\n",
    "for trial in range(297):\n",
    "    for frame in range(1141):\n",
    "        grouped_data[trial, frame, :] = 2 * int((binned_data[trial, frame, :] == 1).any()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa076e19-d81d-4bb9-9e5f-72a7c5b4bb1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data.shape =  (297, 1141, 1)\n",
      "trial #    | percentage belonging to class 1\n",
      "---------------------------------------------\n",
      "trial #  0 | 66.26 %\n",
      "trial #  1 | 69.06 %\n",
      "trial #  2 | 67.92 %\n",
      "trial #  3 | 71.08 %\n",
      "trial #  4 | 68.97 %\n",
      "trial #  5 | 68.27 %\n",
      "trial #  6 | 66.87 %\n",
      "trial #  7 | 65.82 %\n",
      "trial #  8 | 67.66 %\n",
      "trial #  9 | 68.19 %\n",
      "---------------------------------------------\n",
      "AVERAGE     | 68.01 %\n",
      "---------------------------------------------\n",
      "68.47 % of the whole data belongs to class 1.\n"
     ]
    }
   ],
   "source": [
    "# print some statistics\n",
    "print('grouped_data.shape = ', grouped_data.shape)\n",
    "\n",
    "avg_spike_perc = 0\n",
    "print('trial #    | percentage belonging to class 1')\n",
    "print('---------------------------------------------')\n",
    "for trial in range(10):\n",
    "    pers = dh.class_percentages(grouped_data[trial, :, :].reshape(-1), [-1, 1])\n",
    "    avg_spike_perc += pers[1]\n",
    "    print('trial #{:3} | {:.2f} %'.format(trial, pers[1]))\n",
    "\n",
    "avg_spike_perc /= 10\n",
    "print('---------------------------------------------')\n",
    "print('AVERAGE     | {:.2f} %'.format(avg_spike_perc))\n",
    "\n",
    "total_perc = np.sum(grouped_data == 1) *100 /(grouped_data.shape[0] * grouped_data.shape[1])\n",
    "print('---------------------------------------------')\n",
    "print('{:.2f} % of the whole data belongs to class 1.'.format(total_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25ab2-987b-40b8-bee2-a4b14c982c29",
   "metadata": {},
   "source": [
    "# Assess the model's performance with random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03442d6e-dfae-4b33-bee6-a4b5bc828a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform(fv):\n",
    "    \"\"\"\n",
    "    Transform to be applied on feature vectors.\n",
    "    \n",
    "    Input: fv\n",
    "    fv - 1xDf torch tensor representing a feature vector\n",
    "    \n",
    "    Output: fvv\n",
    "    fvv - 1xDf' torch tensor representing the transformed feature vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # for faster run and less memory usage\n",
    "    fvv = fv[::2]\n",
    "    \n",
    "    # for numerical stability during GD\n",
    "    # fvv = fvv * 10\n",
    "    \n",
    "    return fvv\n",
    "\n",
    "# data retrieval params\n",
    "data_params = {'func': dh.datapoint_numpy, 'lbl_func': dh.get_labels, 'features_dp': '../../data/features/sift3d/fvs_s1/', \\\n",
    "               'spike_data': grouped_data, 'group_id': 0, 'transform': transform, 'ind_min': 1*1141+0, 'ind_max': 2*1141-1}\n",
    "\n",
    "# graph construction and penalty term parameters\n",
    "sg_params = {'mu': 30, 'Dt': None, 'Dv':0, 'Dvt':2000, \\\n",
    "             'cnstr_method_tt': 'time', 'cnstr_method_vv': 'time', 'cnstr_method_vt': 'time',\\\n",
    "             'train_t': None, 'val_t': None, \\\n",
    "             'edges_tt':None, 'edges_vv':None, 'edges_vt':None, }\n",
    "\n",
    "# gradient descent parameters\n",
    "sg_opt_params = { 'epsilon0':1, 'epsilon_decay':0.5, 'epsilon_jump': 2, \\\n",
    "                'num_its':16, 'check_freq':1, 'print_checks':False, 'Theta0':None, \\\n",
    "                'force_all_its': True, 'threshold': 0.01}\n",
    "\n",
    "# randomization parameters\n",
    "rnd_params = {'train_sizes': [50, 100, 150, 200, 250, 300], 'val_sizes': [10], 'train_its': 5, 'val_its': 10, 'seed': None}\n",
    "\n",
    "# parameters to visualize the optimized M\n",
    "f_sz = 384 # must match data_params\n",
    "xloc = np.broadcast_to(np.arange(f_sz), (f_sz, f_sz))\n",
    "yloc = xloc.T\n",
    "fig_params = {'rmark_th': 80, 'f_sz': f_sz, 'xloc': xloc, 'yloc': yloc}\n",
    "\n",
    "# path to save the results\n",
    "res_path = '../../data/experiments/sift3d/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb32f59-072f-4078-924d-8523416c1227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_num_res, val_num_err = asg.assess_sg_model(data_params, sg_params, sg_opt_params, rnd_params, fig_params, res_path)\n",
    "asg.plot_curves(rnd_params, sg_params, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload sg package\n",
    "# import importlib\n",
    "# importlib.reload(asg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac8c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
